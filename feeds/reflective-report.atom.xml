<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>FINA4350 Student Blog 2025 - Reflective Report</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/" rel="alternate"></link><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/feeds/reflective-report.atom.xml" rel="self"></link><id>https://buehlmaier.github.io/FINA4350-student-blog-2025-01/</id><updated>2025-04-29T20:00:00+08:00</updated><entry><title>Using Sentiment Score in Earning Call Transcripts to Predict Stock Returns (by Group "Four Guys")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/using-sentiment-score-in-earning-call-transcripts-to-predict-stock-returns-by-group-four-guys.html" rel="alternate"></link><published>2025-04-29T20:00:00+08:00</published><updated>2025-04-29T20:00:00+08:00</updated><author><name>FINA4350 Students 2025</name></author><id>tag:buehlmaier.github.io,2025-04-29:/FINA4350-student-blog-2025-01/using-sentiment-score-in-earning-call-transcripts-to-predict-stock-returns-by-group-four-guys.html</id><summary type="html">&lt;p&gt;By Group "Four Guys"&lt;/p&gt;
&lt;h2&gt;Introducing a new parameter for stock prediction model&lt;/h2&gt;
&lt;p&gt;With the earning call transcript data that we have collected, we want to quantify this unstructured text data into new numerical data that we can use in our linear regression model to predict stock returns. While conventional models …&lt;/p&gt;</summary><content type="html">&lt;p&gt;By Group "Four Guys"&lt;/p&gt;
&lt;h2&gt;Introducing a new parameter for stock prediction model&lt;/h2&gt;
&lt;p&gt;With the earning call transcript data that we have collected, we want to quantify this unstructured text data into new numerical data that we can use in our linear regression model to predict stock returns. While conventional models rely on structured data like P/E ratios, trading volume, and EPS, we hypothesize that the tone of management discussions may contain signals that can improve the prediction accuracy.&lt;/p&gt;
&lt;p&gt;&lt;img alt="PCA Analysis" src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/images/Four-Guys_02_Tone.jpeg"&gt;&lt;/p&gt;
&lt;h2&gt;Our base model&lt;/h2&gt;
&lt;p&gt;We planned to use these traditional parameters to build our base model (Multilayer Perceptron):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;P/E ratio (to see if company is currently overvalued or undervalued)&lt;/li&gt;
&lt;li&gt;Average volatility (to account for stock’s volatility)&lt;/li&gt;
&lt;li&gt;Trading volume (to see liquidity indicator and market participation)&lt;/li&gt;
&lt;li&gt;EPS (previous earnings performance)&lt;/li&gt;
&lt;li&gt;Market returns (to account for overall market performance)&lt;/li&gt;
&lt;li&gt;Number of positive words and negative words (using the Loughran-McDonald Master Dictionary)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These parameters will be used to train the base model before adding in sentiment scores to ideally improve the model’s accuracy.&lt;/p&gt;
&lt;h2&gt;Generating sentiment score&lt;/h2&gt;
&lt;p&gt;One of the challenges we face is generating an accurate and useful sentiment score. There are several libraries that we researched before choosing the best one for our project.&lt;/p&gt;
&lt;p&gt;VADER - this library is designed to analysis social media sentiments with the ability to better understand sarcasm and slang, but in our earning call transcript, most of the conversations for format with technical terms that VADER may not understand well&lt;/p&gt;
&lt;p&gt;FinBERT- this library is fine-tuned and pre-trained with financial knowledge which is highly suited for our earning call data, however it is more computationally intensive than other models&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_vader_sentiment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Compute VADER sentiment score.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;vader_analyzer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polarity_scores&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;compound&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_finbert_sentiment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Compute FinBERT sentiment score.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Truncate text to max length (512 tokens)&lt;/span&gt;
        &lt;span class="n"&gt;inputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;finbert_tokenizer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;return_tensors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;truncation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;padding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;no_grad&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
            &lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;finbert_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;logits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;
        &lt;span class="n"&gt;probabilities&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;softmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logits&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="c1"&gt;# FinBERT labels: positive, negative, neutral&lt;/span&gt;
        &lt;span class="n"&gt;sentiment_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;probabilities&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# Positive - Negative&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;sentiment_score&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;Exception&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Error computing FinBERT sentiment: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_combined_sentiment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Combine VADER and FinBERT sentiments (weighted average).&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;vader_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_vader_sentiment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;finbert_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;compute_finbert_sentiment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;vader_score&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;finbert_score&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
    &lt;span class="c1"&gt;# Weighted average (e.g., 50% VADER, 50% FinBERT)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;vader_score&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;finbert_score&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Our code calculates a combined sentiment score by averaging results from both VADER and FinBERT to leverage their complementary strengths:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;VADER (fast, slang/sarcasm-aware) provides a baseline sentiment score.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;FinBERT (financial-domain specialized) adds nuanced understanding of financial jargon.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The final score (compute_combined_sentiment) is a 50/50 weighted average of both, balancing speed and domain relevance while handling edge cases (empty text, errors) gracefully.&lt;/p&gt;
&lt;h2&gt;Visualization&lt;/h2&gt;
&lt;p&gt;One of the challenges we face when using deep learning models is the visualization problem. We train our model with seven inputs from the previous week: weekly return, PE ratio, EPS ratio, bid volume, ask volume, volatility, and, most importantly, sentiment scores from earnings calls. The model's output is the predicted weekly return for the week following the publication of the earnings call transcript. However, visualizing the relationship between these seven inputs and the target variable (the upcoming weekly return) is difficult, as it cannot be easily represented in 2D or even 3D graphs.&lt;/p&gt;
&lt;p&gt;&lt;img alt="PCA Analysis" src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/images/Four-Guys_02_PCA.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;We therefore attempted an 8 by 8 pairwise plot, but it's challenging to gain clear insights due to the numerous variables involved. Additionally, these variables may interact with each other, further complicating their collective influence on the output. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Pairwise Plot" src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/images/Four-Guys_02_Pairwise-Plot.png"&gt;&lt;/p&gt;
&lt;p&gt;Ultimately, we chose to use Principal Component Analysis to reduce the dimensionality from 7 to 2. We also color-coded the output variables to enhance the visualization of our model's predictions.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Principal Component Analysis Visualization" src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/images/Four-Guys_02_PCA-Visualization.jpeg"&gt;&lt;/p&gt;</content><category term="Reflective Report"></category><category term="Group Four Guys"></category></entry><entry><title>Evaluating LLMs as an Alternative for VADER and FinBERT in Financial Sentiment Analysis (by Group "FinText")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/evaluating-llms-as-an-alternative-for-vader-and-finbert-in-financial-sentiment-analysis-by-group-fintext.html" rel="alternate"></link><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><author><name>FINA4350 Students 2025</name></author><id>tag:buehlmaier.github.io,2025-04-28:/FINA4350-student-blog-2025-01/evaluating-llms-as-an-alternative-for-vader-and-finbert-in-financial-sentiment-analysis-by-group-fintext.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Sentiment analysis has become an essential tool in trading, extracting market-moving signals from data sources like news articles. Our News-driven NLP Quant project currently employs two established methods: VADER, a lexicon-based approach, and FinBERT, a financial domain-specific model. With the emergence of sophisticated large language models (LLMs), we must …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Sentiment analysis has become an essential tool in trading, extracting market-moving signals from data sources like news articles. Our News-driven NLP Quant project currently employs two established methods: VADER, a lexicon-based approach, and FinBERT, a financial domain-specific model. With the emergence of sophisticated large language models (LLMs), we must examine whether these advanced systems can surpass traditional techniques in financial sentiment analysis.&lt;/p&gt;
&lt;p&gt;This comparative study analyses three real-world financial news cases to assess the strengths and limitations of each approach. The findings provide actionable insights for practitioners considering sentiment analysis methodologies. For this analysis, we utilised DeepSeek as our test LLM.&lt;/p&gt;
&lt;h2&gt;Comparative Case Analysis&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Approach&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Case 1&lt;/th&gt;
&lt;th&gt;Case 2&lt;/th&gt;
&lt;th&gt;Case 3&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;VADER&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Title sentiment&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.1531&lt;/td&gt;
&lt;td&gt;-0.4938&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Summary sentiment&lt;/td&gt;
&lt;td&gt;-0.5106&lt;/td&gt;
&lt;td&gt;-0.2960&lt;/td&gt;
&lt;td&gt;0.3818&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;FinBERT&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Title sentiment&lt;/td&gt;
&lt;td&gt;0.9480&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Summary sentiment&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;-0.9954&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Title sentiment&lt;/td&gt;
&lt;td&gt;0.35&lt;/td&gt;
&lt;td&gt;-0.65&lt;/td&gt;
&lt;td&gt;-0.70&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Summary sentiment&lt;/td&gt;
&lt;td&gt;0.60&lt;/td&gt;
&lt;td&gt;-0.55&lt;/td&gt;
&lt;td&gt;-0.45&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Case 1: Market Reaction to Geopolitical News&lt;/h3&gt;
&lt;p&gt;The article &lt;strong&gt;"Wall Street Rallies as West Hits Russia with New Sanctions"&lt;/strong&gt; presented an interesting test case for sentiment analysis. VADER produced a neutral score (0.00) for the title and negative (-0.51) for the summary, failing to capture the market's positive interpretation of events. FinBERT correctly identified positive sentiment with high confidence, while DeepSeek produced scores of +0.35 for the title and +0.60 for the summary. All three methods captured different aspects of the complex sentiment landscape.&lt;/p&gt;
&lt;h3&gt;Case 2: Macroeconomic Downturn Report&lt;/h3&gt;
&lt;p&gt;Analysis of &lt;strong&gt;"Weak Manufacturing Drags Down Q3 GDP Growth"&lt;/strong&gt; revealed important differences in approach. VADER produced an unexpected positive score (+0.15) for the negative-leaning headline. FinBERT generated a strongly negative classification, while DeepSeek produced scores of -0.65 for the title and -0.55 for the summary. This case highlights how different methodologies handle gradations in negative sentiment.&lt;/p&gt;
&lt;h3&gt;Case 3: Corporate Earnings Announcement&lt;/h3&gt;
&lt;p&gt;The Zoom earnings report demonstrated the challenges of interpreting mixed financial signals. VADER produced contradictory scores (-0.49 for the title versus +0.38 for the summary). FinBERT consistently classified both components as negative, while DeepSeek provided scores of -0.70 for the title and -0.45 for the summary. This suggests that while transformer-based models may be more consistent than lexicon approaches, they can differ significantly in their interpretation of ambiguous cases.&lt;/p&gt;
&lt;h2&gt;Methodological Comparison&lt;/h2&gt;
&lt;h3&gt;Accuracy and Nuance&lt;/h3&gt;
&lt;p&gt;FinBERT and DeepSeek captured contextual clues that VADER missed. The LLM tended to provide graduated scores (e.g., –0.55 instead of a hard –1), which can be valuable for position sizing.&lt;/p&gt;
&lt;h3&gt;Computational Considerations&lt;/h3&gt;
&lt;p&gt;FinBERT strikes a favorable balance between speed and accuracy, processing documents within milliseconds while maintaining strong domain-specific relevance. VADER offers the fastest processing without requiring a GPU, but at the cost of reduced accuracy. LLMs, while highly capable in this task, typically demand significantly greater computational resources and project budget.&lt;/p&gt;
&lt;h3&gt;Interpretability&lt;/h3&gt;
&lt;p&gt;FinBERT is open-source, allowing inspection of its token-level logits. In contrast, closed-weight LLMs offer limited transparency. However, both FinBERT and LLMs remain largely black-box models compared to the rule-based and inherently interpretable VADER.&lt;/p&gt;
&lt;h2&gt;Practical Recommendations&lt;/h2&gt;
&lt;p&gt;For research baselines and prototyping, FinBERT offers a robust solution, delivering solid accuracy with modest computational overhead. When time and budget permit, with the need for fine-grained sentiment insights, LLMs provide powerful capabilities for deeper analysis. In some cases, a hybrid cascade approach is effective where FinBERT handle the first pass and escalating only complex or ambiguous cases to an LLM for finer interpretation. VADER, while limited in financial nuance, remains useful for rapid prototyping or preliminary checks, especially when GPU resources are unavailable.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This comparative analysis demonstrates that both LLMs and fine-tuned, smaller models like FinBERT have distinct roles in financial sentiment analysis. While LLMs, as evidenced by articles like Fatouros et al. (2023), excel at processing nuanced financial texts and show superior correlation with market reactions, FinBERT remains indispensable when budget constraints and computational efficiency are critical.&lt;/p&gt;
&lt;p&gt;Future work should focus on developing hybrid approaches that leverage the strengths of both specialised financial models and general-purpose LLMs, along with standardised evaluation frameworks for comparing different sentiment analysis methodologies. The financial NLP community would benefit from continued research into more efficient LLM architectures and a better understanding of how these different approaches complement each other in real-world applications.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;Fatouros, G., Soldatos, J., Kouroumali, K., Makridis, G., &amp;amp; Kyriazis, D. (2023)&lt;br&gt;
Transforming sentiment analysis in the financial domain with ChatGPT.&lt;br&gt;
Machine Learning with Applications, 12, 100508. &lt;a href="https://doi.org/10.1016/j.mlwa.2023.100508"&gt;https://doi.org/10.1016/j.mlwa.2023.100508&lt;/a&gt;&lt;/p&gt;</content><category term="Reflective Report"></category><category term="Group FinText"></category></entry><entry><title>Blog 2 (by Group "NLPredict")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/blog-2-by-group-nlpredict.html" rel="alternate"></link><published>2025-04-20T20:00:00+08:00</published><updated>2025-04-20T20:00:00+08:00</updated><author><name>FINA4350 Students 2025</name></author><id>tag:buehlmaier.github.io,2025-04-20:/FINA4350-student-blog-2025-01/blog-2-by-group-nlpredict.html</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The following blog covers the developmental progress of NLPredict by April 21st 2025 and will explore the various checkpoints, difficulties and breakthroughs our team has made throughout the developmental process. In addition, this blog will also reflect on various mistakes made within NLPredict’s programming as well as various …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The following blog covers the developmental progress of NLPredict by April 21st 2025 and will explore the various checkpoints, difficulties and breakthroughs our team has made throughout the developmental process. In addition, this blog will also reflect on various mistakes made within NLPredict’s programming as well as various challenges that our team has faced outside of the technical aspects of this project.&lt;/p&gt;
&lt;h1&gt;Progress since the Previous Blog&lt;/h1&gt;
&lt;p&gt;Currently our team has finished programming and implementing the final phases of NLPredict’s model, achieving a much higher accuracy in prediction than our initial values. We have also achieved most of our targets in terms of backtesting and evaluating NLPredict’s model, both from an accuracy and efficiency perspective. Our research into the market has been helpful in narrowing down the sources of errors as well as determining whether or not certain stocks and data sources should be included into training the model.&lt;/p&gt;
&lt;p&gt;NLPredict’s model has now incorporated the use of other pieces of financial data such as the Standard and Poor 500’s (S&amp;amp;P 500) historical price, as well as various indices such as the Hang Seng Index (HSI) in order to corroborate and verify the accuracy of NLPredict’s predictions. Initially, there was some discussion as to whether or not this decision would go against the goal of making Natural Language Processing (NLP) and Sentiment Analysis play a more active role in making stock price predictions. However, after a lengthy discussion, it was decided that using other pieces of financial data in support of NLP did not go against the original aim of this project.&lt;/p&gt;
&lt;p&gt;Jason has been researching the market and modern uses of Sentiment Analysis which has helped with determining the effect of market sentiment on the prices of Stocks, as well as recording certain tasks which needed to be done for the project. This was particularly useful when considering short-term price-shocks caused by sudden events or worries (such as natural disasters) to decrease or increase the effects of market sentiment on particular stocks. Using this information as an opportunity, we have also discussed how to deal with such anomalies in the market and our data sources. Initially our team wanted to leave the problem as is, given it was not a problem affecting a large number of stocks. However, after some reconsideration, it was decided to make some minor changes to the model’s data inputs in favor of a more complete final product.&lt;/p&gt;
&lt;p&gt;Herbert has been mostly in charge of programming and fine tuning our model while the rest of our team has been working on backtesting and debugging the model. During this process, we have looked at using different accuracy metrics to determine the validity of NLPredict’s predictions. Ultimately, we have determined that using mainly financial and regression metrics would be the most efficient as it would be the easiest to explain and therefore the most accessible to the wider market.&lt;/p&gt;
&lt;p&gt;A sample of the code we have used can be found below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;evaluate_predictions&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;regression&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Evaluate predictions with comprehensive metrics&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;metrics&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;model_type&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;regression&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Regression metrics&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mse&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_squared_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rmse&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mse&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mae&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean_absolute_error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;r2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r2_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Financial specific metrics&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;accuracy_direction&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Direction accuracy&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mean_return&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sign_consistency&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sign&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="c1"&gt;# Risk-adjusted metrics&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sharpe&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# Classification metrics&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;accuracy&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;precision&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;binary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;recall&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;binary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;f1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;average&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;binary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# Class balance&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;positive_ratio&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;metrics&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;predicted_positive_ratio&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;metrics&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;While initially it seemed that using only two indicators of accuracy was insufficient and that having more indicators would give a more holistic view of the model’s accuracy, in hindsight, it seems that having fewer indicators has aided us in judging whether the model’s predictions were sufficient.&lt;/p&gt;
&lt;p&gt;A key development of the project was the narrowing down of data sources that we have incorporated into the NLPredict’s data input. Initially, NLPredict was taking in data from various sources. However, this came with the problem that there were certain repetitions that were entering its predictions which led it to favour one sentiment in particular. As such, the decision was made to narrow down its data input to be from mainstream sources such as Yahoo Finance, rather than various financial news sources. While this decision was made, it was agreed that it was a temporary solution rather than a permanent one. The team has unanimously recognised that the benefits in an increase in data sources and inputs would far outweigh the demerits. However, it was noted that it would require much more engineering and testing to determine whether or not a data source or input was valuable or useful for NLPredict’s Training and predictions.&lt;/p&gt;
&lt;p&gt;The initial code which uses various sources can be found below with most of the code omitted in order to demonstrate the breadth of sources used. The following list is an example of some of the sources used.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_news_from_finviz&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ticker&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_articles&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Get news from Finviz with robust parsing&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://finviz.com/quote.ashx?t=&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;ticker&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Fetching news from Finviz: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_news_from_wsj&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ticker&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_articles&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Get news from Wall Street Journal Search&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://www.wsj.com/search?query=&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;ticker&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Fetching news from WSJ: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_news_from_marketwatch&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ticker&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_articles&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Get news from MarketWatch&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://www.marketwatch.com/investing/stock/&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;ticker&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Fetching news from MarketWatch: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_news_from_reuters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;company_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_articles&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Get news from Reuters&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://www.reuters.com/search/news?blob=&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;quote_plus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;company_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Fetching news from Reuters: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_news_from_bloomberg&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;company_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_articles&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Get news from Bloomberg (note: might have limited success due to paywall)&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://www.bloomberg.com/search?query=&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;quote_plus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;company_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Fetching news from Bloomberg: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_news_from_financial_times&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ticker&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;company_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_articles&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Get news from Financial Times&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# Try company name first, then ticker&lt;/span&gt;
    &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://www.ft.com/search?q=&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;quote_plus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;company_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Fetching news from Financial Times: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After discussion and testing, we have narrowed down the data sources, some of which can be found below.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_news_from_yahoo_finance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ticker&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_articles&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Get news from Yahoo Finance with robust parsing&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://finance.yahoo.com/quote/&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;ticker&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;/news&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Fetching news from Yahoo Finance: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_news_from_bloomberg&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;company_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_articles&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Get news from Bloomberg (note: might have limited success due to paywall)&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://www.bloomberg.com/search?query=&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;quote_plus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;company_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Fetching news from Bloomberg: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_news_from_financial_times&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ticker&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;company_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_articles&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Get news from Financial Times&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# Try company name first, then ticker&lt;/span&gt;
    &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://www.ft.com/search?q=&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;quote_plus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;company_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Fetching news from Financial Times: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Looking forward, some other developments we would like to make would be to incorporate more data types that would accentuate the use of Sentiment Analysis and NLP in stock price predictions. Currently, development on this front has been low due to a lack of ideas regarding such data outside of numerical and historical data. Despite this, the team has been satisfied with the work that was accomplished on this project in the time given.&lt;/p&gt;</content><category term="Reflective Report"></category><category term="Group NLPredict"></category></entry><entry><title>Decoding Crypto Volatility: Leveraging NLP to Predict TerraLuna's Market Performance (by Group "DeepText Analysts")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/DeepText%20Analysts.html" rel="alternate"></link><published>2025-03-24T23:59:00+08:00</published><updated>2025-03-24T23:59:00+08:00</updated><author><name>FINA4350 Students 2025</name></author><id>tag:buehlmaier.github.io,2025-03-24:/FINA4350-student-blog-2025-01/DeepText Analysts.html</id><summary type="html">&lt;p&gt;By Group "DeepText Analysts"&lt;/p&gt;
&lt;h1&gt;The Digital Gold Rush: Why Cryptocurrency Analysis Matters Now&lt;/h1&gt;
&lt;p&gt;Our team, DeepText Analysts, is investigating whether natural language processing (NLP) can predict cryptocurrency performance, with a specific focus on TerraLuna. This research carries significant implications due to the cryptocurrency industry's explosive growth, increasing mainstream adoption by …&lt;/p&gt;</summary><content type="html">&lt;p&gt;By Group "DeepText Analysts"&lt;/p&gt;
&lt;h1&gt;The Digital Gold Rush: Why Cryptocurrency Analysis Matters Now&lt;/h1&gt;
&lt;p&gt;Our team, DeepText Analysts, is investigating whether natural language processing (NLP) can predict cryptocurrency performance, with a specific focus on TerraLuna. This research carries significant implications due to the cryptocurrency industry's explosive growth, increasing mainstream adoption by businesses, persistent market volatility, and the need for better investment decision support tools.&lt;/p&gt;
&lt;p&gt;The intersection of NLP and cryptocurrency markets represents a promising frontier in fintech research, with practical applications for risk management and investment strategy development in this rapidly evolving but highly unpredictable financial landscape.&lt;/p&gt;
&lt;p&gt;By applying sentiment analysis to social media discussions and other information sources, we aim to develop models that might forecast crypto market movements. The dramatic collapse of TerraLuna serves as our central case study, raising the question: Could sentiment analysis have provided early warning signals before the crash?&lt;/p&gt;
&lt;h4&gt;About TerraLuna&lt;/h4&gt;
&lt;p&gt;TerraLuna is a decentralised blockchain platform containing TerraUSD, an algorithmic stablecoin that was backed by LUNA, a native token which provides an arbitrage opportunity by absorbing the short-term volatilities of TerraUSD. TerraUSD’s unique concept of securing stability with LUNA instead of normal asset reserves appeared successful as the 8th largest market capitalisation in cryptocurrency in April 2022. Luna’s sudden crash on May 9th, 2022 was caused by over $2 billion worth of UST being unstaked which depegged the stablecoin and caused crypto exchanges to delist LUNA and UST pairings. This made LUNA worthless, resulting in significant consequences to the highly volatile cryptocurrency market and Luna investors.  &lt;/p&gt;
&lt;p&gt;Due to TerraLuna’s strong past performance, the company’s loyal fans, known as Lunatics used to frequently share their thoughts and discussions on Reddit. Thus, given the copious amount of available data and the complete price history on the growth and downfall of TerraLuna, we plan to use this information to train our model which we hope can analyse and predict cryptocurrency performance based on public sentiment on other cryptocurrencies.&lt;/p&gt;
&lt;h1&gt;Our Methodology&lt;/h1&gt;
&lt;p&gt;To start our data workflow, we have carefully considered the various types of data sources to establish as our foundational framework. We have structured our thought process around several key factors: the nature of the data source, the category of cryptocurrency, and the timeline of our data, and the methodology we employ to measure sentiment analysis. Each of these elements plays a critical role in ensuring the robustness and accuracy of our research. &lt;/p&gt;
&lt;p&gt;Firstly, with our data source, we decided to start working on Reddit posts, an online social media platform which offers a rich database on a diverse range of topics. We chose to focus on the comment section which makes it particularly easy to find insightful discussions and news articles related to cryptocurrency, especially on the subject of TerraLuna. Next, we established a specific timeline for our data collection, focusing on posts from the year 2020, when cryptocurrency was gaining popularity, to 2022, just prior to the collapse of TerraLuna. With this timeframe, we can properly evaluate whether our results align with the actual historical outcomes of the cryptocurrency market, ensuring the relevance and accuracy of our analysis. &lt;/p&gt;
&lt;p&gt;Taking all these factors into account, this serves as our foundational base case. Our product is designed to offer flexibility, enabling the application of this methodology to any cryptocurrency by leveraging diverse data sources and analytical approaches, such as dictionary-based methods, deep learning techniques, or Python packages. Once our system is established using this base case, we will expand our exploration to incorporate a variety of scenarios and methodologies.&lt;/p&gt;
&lt;h1&gt;Breaking the Code: Our Multi-Faceted Approach to Crypto Prediction&lt;/h1&gt;
&lt;h4&gt;Model 1: Dictionary-Based Sentiment Analysis&lt;/h4&gt;
&lt;p&gt;We're analyzing thousands of Reddit threads related to TerraLuna using lexicon-based sentiment analysis techniques. This approach leverages the Loughran-McDonald Dictionary, which was specifically developed for financial text analysis and captures finance-specific terminology that general sentiment dictionaries often misclassify. The dictionary categorizes words into six sentiment categories (positive, negative, uncertainty, litigious, strong modal, and constraining), allowing us to detect nuanced financial sentiment beyond simple polarity. Each comment is processed to extract these specialized sentiment metrics, enabling us to track shifts in investor confidence, uncertainty, and regulatory concerns within the TerraLuna community over time. We're particularly focused on identifying sentiment patterns that preceded major price volatility during the collapse.&lt;/p&gt;
&lt;h4&gt;Model 2: Advanced Deep Learning Architecture&lt;/h4&gt;
&lt;p&gt;We're developing sophisticated neural network models—including BERT (Bidirectional Encoder Representations from Transformers) and FinBERT (Financial domain-specific BERT)—that can capture nuanced relationships between language patterns and market movements. These transformer-based models excel at understanding context and can be fine-tuned on cryptocurrency-specific language. Unlike dictionary methods, these approaches can recognize complex linguistic patterns, sarcasm, and emerging terminology common in crypto communities.&lt;/p&gt;
&lt;h1&gt;Potential Additional Methods to Explore&lt;/h1&gt;
&lt;h4&gt;Topic Modeling with LDA (Latent Dirichlet Allocation)&lt;/h4&gt;
&lt;p&gt;By implementing topic modeling, we could identify emerging discussion themes that correlate with market shifts. This would help us understand not just sentiment polarity but the specific concerns driving community reactions.&lt;/p&gt;
&lt;h4&gt;Time Series Forecasting with LSTM Networks&lt;/h4&gt;
&lt;p&gt;Long Short-Term Memory networks could help us better model the temporal dynamics between sentiment shifts and price movements, accounting for both immediate and delayed effects.&lt;/p&gt;
&lt;h4&gt;Hybrid NLP-Technical Analysis&lt;/h4&gt;
&lt;p&gt;Combining our NLP insights with traditional technical indicators (RSI, MACD, Bollinger Bands) could yield a more comprehensive prediction model that considers both market psychology and price action patterns.&lt;/p&gt;
&lt;h4&gt;Transfer Learning from Related Assets&lt;/h4&gt;
&lt;p&gt;We could explore transfer learning techniques to leverage patterns discovered in other cryptocurrencies to enhance our TerraLuna predictions, potentially identifying universal sentiment indicators across the crypto market.&lt;/p&gt;
&lt;h4&gt;Named Entity Recognition for Key Influencers&lt;/h4&gt;
&lt;p&gt;Implementing named entity recognition could help us identify and track key influencers whose opinions disproportionately impact market sentiment and price movements.&lt;/p&gt;
&lt;h1&gt;Current Progress&lt;/h1&gt;
&lt;p&gt;Our data collection phase has been fascinating. We've gathered Reddit discussions about TerraLuna. This code shows how we used the Reddit API for data collection:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;praw&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;openpyxl&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt;

&lt;span class="c1"&gt;# initialize Reddit API&lt;/span&gt;
&lt;span class="n"&gt;reddit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;praw&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Reddit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;client_id&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ZExuVDrnuon1q8SWA__2fw&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;client_secret&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;TENjvYzdpCZV2tA8gwA_8bEkyNfghg&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;user_agent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;ImportanceAsleep6865&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;subreddit&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;reddit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subreddit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cryptocurrency&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;query&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Terra OR Luna&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;search_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subreddit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# time range(2021.1 - 2022.6)&lt;/span&gt;
&lt;span class="n"&gt;start_timestamp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2019&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timestamp&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;  &lt;span class="c1"&gt;# 2019-01-01&lt;/span&gt;
&lt;span class="n"&gt;end_timestamp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2022&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timestamp&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;  &lt;span class="c1"&gt;# 2022-06-30&lt;/span&gt;

&lt;span class="c1"&gt;# filter and save to DataFrame&lt;/span&gt;
&lt;span class="n"&gt;filtered_posts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Title&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Post URL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Created At&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[]}&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;post&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;search_results&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;start_timestamp&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;created_utc&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;end_timestamp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;filtered_posts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Title&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;filtered_posts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Post URL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;permalink&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;filtered_posts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Created At&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromtimestamp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;post&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;created_utc&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# convert to Pandas DataFrame&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filtered_posts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;terra_luna_posts.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;utf-8&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; There are &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; of post satisified the requirement and saved to terra_luna_posts.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then we created visualizations including word clouds that reveal common themes and concerns among community members. This is important for us as we might have to filter based on themes/words which comments in the dataset are actually relevant. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Picture showing Word Cloud" src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/images/DeepText-Analysts-01-word-cloud.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Words like "lost," "deleted," "shit," "ponzi," and "scam" indicate negativity, possibly referring to financial losses or scams."Good," "great," and "better" suggest some positive views, but they are relatively smaller.
Frequent mentions of "money," "price," "value," and "market" indicate concerns about investment performance.Words like "buy," "sell," "risk," "stable," and "USD" imply discussions about trading, stability, and financial decisions.
We've also compiled historical price-volume data to correlate with our sentiment metrics.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Picture showing Price Volume" src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/images/DeepText-Analysts-01-price-volume-chart.png"&gt;&lt;/p&gt;
&lt;p&gt;Based on this figure, we can see the price surge and fall of Terra Luna. The price experienced a significant rise from early 2021, peaking around late 2021 or early 2022. After reaching its peak, the price exhibited some volatility but remained high until mid-2022. A sharp and dramatic collapse occurred around mid-2022, where the price dropped to near zero and remained flat afterward.
Trading volume fluctuated throughout the period, with occasional spikes corresponding to price movements.There was a massive spike in trading volume around the time of the price crash, indicating panic selling or forced liquidations.After the collapse, trading volume also significantly decreased, suggesting reduced interest or market activity.  &lt;/p&gt;
&lt;p&gt;Initial tests using our dictionary method have yielded intriguing but statistically insignificant results:  &lt;/p&gt;
&lt;p&gt;We started off simple by just calculating a negativity score for each comment, based on the classification of the Loughran-McDonald Dictionary. We then aggregated and averaged those scores for each day and compared it to the price movement.  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;kagglehub&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;nltk&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;nltk.tokenize&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;word_tokenize&lt;/span&gt;
&lt;span class="n"&gt;nltk&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;punkt_tab&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# For tokenization&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;ast&lt;/span&gt;  &lt;span class="c1"&gt;# For safely parsing string lists&lt;/span&gt;

&lt;span class="c1"&gt;# Step 1: Load Loughran-McDonald Dictionary&lt;/span&gt;
&lt;span class="n"&gt;lm_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Loughran-McDonald_Dictionary.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Extract negative words (where &amp;#39;Negative&amp;#39; column &amp;gt; 0)&lt;/span&gt;
&lt;span class="n"&gt;negative_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lm_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;lm_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Negative&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Word&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Loaded &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;negative_words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; negative words from LM Dictionary&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Step 2: Load the test dataset from BTC.csv&lt;/span&gt;
&lt;span class="c1"&gt;# Replace &amp;#39;BTC.csv&amp;#39; with your actual file path if different&lt;/span&gt;
&lt;span class="n"&gt;headlines_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;BTC.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Step 3: Define sentiment scoring function&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_lm_negativity_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# Tokenize the headline into words&lt;/span&gt;
    &lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;word_tokenize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;total_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Count negative words&lt;/span&gt;
    &lt;span class="n"&gt;neg_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;words&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;word&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;negative_words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Calculate negativity score (proportion of negative words)&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;total_words&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;neg_count&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;total_words&lt;/span&gt;  &lt;span class="c1"&gt;# Range: 0 to 1 (higher = more negative)&lt;/span&gt;

&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;[# Step 4: Function that applies &amp;quot;get_lm_negativity_score&amp;quot; on each comment]&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

&lt;span class="c1"&gt;# Step 5: Calculate daily average negativity score&lt;/span&gt;
&lt;span class="c1"&gt;# Convert Comment Time to datetime and extract date only&lt;/span&gt;
&lt;span class="n"&gt;comments_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Comment Date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;comments_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Comment Time&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;
&lt;span class="n"&gt;daily_negativity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;comments_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Comment Date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;negativity_score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reset_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;daily_negativity&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rename&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;negativity_score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;avg_negativity_score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Step 6: Load price data&lt;/span&gt;
&lt;span class="c1"&gt;# Replace &amp;#39;terra-historical-day-data-all-tokeninsight.csv&amp;#39; with your actual file path&lt;/span&gt;
&lt;span class="n"&gt;price_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;terra-historical-day-data-all-tokeninsight.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Step 7: Merge daily negativity scores with price data&lt;/span&gt;
&lt;span class="c1"&gt;# Ensure date formats match (convert price Date to date)&lt;/span&gt;
&lt;span class="n"&gt;price_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;price_df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;
&lt;span class="n"&gt;merged_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;daily_negativity&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;price_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;left_on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Comment Date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;right_on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;left&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="sd"&gt;&amp;#39;&amp;#39;&amp;#39;[...]&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;→ Our first trial analyzing all Reddit comments produced an R-squared value of only &lt;strong&gt;0.003&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These preliminary result suggested that our approach requires refinement. We did two smaller adjustments:  &lt;/p&gt;
&lt;p&gt;a) Cutting off post-crash data in the regression analysis, as it might be insignificant noise at a time when the price is not really moving anymore:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;statsmodels.api&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;sm&lt;/span&gt;

&lt;span class="c1"&gt;# Step 1: Load the Excel file with merged data&lt;/span&gt;
&lt;span class="c1"&gt;# Replace &amp;#39;daily_negativity_and_prices.xlsx&amp;#39; with your actual file path if different&lt;/span&gt;
&lt;span class="n"&gt;input_file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;daily_negativity_and_prices.xlsx&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_excel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input_file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Step 2: Set timeframe (replace with desired start and end dates)&lt;/span&gt;
&lt;span class="n"&gt;start_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2022-01-01&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# Example start date&lt;/span&gt;
&lt;span class="n"&gt;end_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2022-05-15&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;    &lt;span class="c1"&gt;# Example end date | important: cutoff after crash&lt;/span&gt;

&lt;span class="c1"&gt;# Filter data based on timeframe&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Comment Date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;start_date&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_datetime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Comment Date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;end_date&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="c1"&gt;# Step 3: Prepare the data for regression&lt;/span&gt;
&lt;span class="c1"&gt;# Independent variable(s): avg_negativity_score&lt;/span&gt;
&lt;span class="c1"&gt;# Dependent variable: Price&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;avg_negativity_score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;  &lt;span class="c1"&gt;# Independent variable&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_constant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Add a constant term for the intercept&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Price&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  &lt;span class="c1"&gt;# Dependent variable&lt;/span&gt;

&lt;span class="c1"&gt;# Step 4: Run multiple regression&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OLS&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# Step 5: Display regression results&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;Multiple Regression Results:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;b) Filtering the comments to make it more likely that they are actually about TerraLuna and not only Bitcoin or completely different topics:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="c1"&gt;# Step 1: Load the Excel file with Reddit comments&lt;/span&gt;
&lt;span class="c1"&gt;# Replace &amp;#39;reddit_comments.xlsx&amp;#39; with your actual file path&lt;/span&gt;
&lt;span class="n"&gt;comments_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_excel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;reddit_selected_rows.xlsx&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Step 2: Transform data to filter comments containing &amp;quot;terra&amp;quot; or &amp;quot;luna&amp;quot; (case-insensitive)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;filter_terra_luna_comments&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Comment Text&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contains&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;terra|luna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;case&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;na&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;filtered_comments_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;filter_terra_luna_comments&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;comments_df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Step 3: Save to a new Excel file&lt;/span&gt;
&lt;span class="n"&gt;filtered_comments_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_excel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;filtered_reddit_selected_rows.xlsx&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;Results saved to &amp;#39;filtered_reddit_selected_rows.xlsx&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;→ Still, our second trial with selected comments similarly showed an R-squared of &lt;strong&gt;0.003&lt;/strong&gt;  &lt;/p&gt;
&lt;p&gt;Hence, we currently have further adjustments in mind:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Trying to use a &lt;strong&gt;multiple discriminant analysis&lt;/strong&gt;: calculating scores for every category of the Loughran-McDonald Dictionary and finding out which (combination of) categories is/are the most accurate predictor of price&lt;/li&gt;
&lt;li&gt;Weighting comments based on community engagement (upvotes/downvotes)&lt;/li&gt;
&lt;li&gt;Incorporating time lag analysis through cross-correlation to identify delayed effects  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After getting the most out of the simple dictionary method, we will try to move on to a deep learning architecture. &lt;/p&gt;
&lt;h1&gt;Technical Challenges and Lessons Learned&lt;/h1&gt;
&lt;p&gt;One significant challenge we've encountered is distinguishing meaningful signals from noise in social media data. Cryptocurrency communities can be particularly reactive and emotional, making sentiment analysis complex.  &lt;/p&gt;
&lt;p&gt;We've learned that simple correlation between sentiment scores and price movements doesn't capture the full relationship. Market dynamics likely involve multiple time scales and feedback loops that require more sophisticated modeling approaches.&lt;/p&gt;
&lt;h1&gt;Next Steps&lt;/h1&gt;
&lt;p&gt;Our team has established a detailed timeline for completing remaining tasks:  &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Refine our data collection processes&lt;/li&gt;
&lt;li&gt;Improve our dictionary-based sentiment analysis model&lt;/li&gt;
&lt;li&gt;Develop and train our deep learning model&lt;/li&gt;
&lt;li&gt;Compare performance of both approaches&lt;/li&gt;
&lt;li&gt;Document findings and prepare presentation  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We're particularly excited about implementing cross-correlation analysis to better understand temporal relationships between sentiment shifts and price movements.&lt;/p&gt;
&lt;h1&gt;Reflections on the Process&lt;/h1&gt;
&lt;p&gt;We learned a lot in the first weeks of working on the project. The most striking lessons are: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It is not easy at all to simply try to predict a price based on the sentiment. The initial major question that comes up is: is sentiment an accurate predictor of price? And if it is, it might still be very hard to get an accurate picture of the sentiment, that is, choosing the “right” sentiment data. Maybe people on Reddit are not moving the price, as they represent only small amounts of money. Is their thought actually relevant? If not, can we actually get the sentiment of those people who are moving the price? Either way, sentiment might still be lagging the price instead of predicting it. This is important to consider for further analysis. We are not only trying to make work something that does work for sure, but we also have to find out if it can work in the first place.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;While the knowledge we learn in this course and the existing Python libraries are of much help in the code work, the real work is still done by getting more familiar with statistical thinking. The issue so far was not to put an idea of a method to work, but rather to find the right method for our case. It was easy for us to calculate negativity scores and get sentiment data, but it turned out to be difficult to make the most of that data, as there are endless methods we could apply to it. Therefore, we saw the need to actually understand what is going on under the hood of the Python libraries we are using. We ran a polynomial-regression method to determine the correlation of the negativity scores and price, but it could well be that only extreme negativity scores are correlating with sharp price movements, and that everything else is simple noise. This would require a completely different model than a polynomial regression to predict price movements.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spending time together and doing regular meetings is absolutely essential and has helped us a lot. Since the beginning of the Semester, we met up almost every week and already developed a lot of great ideas together we probably would have not come up with alone. We also managed to distribute work effectively and are therefore confident that together we will be able to develop a model with more significant predictive power in the coming weeks. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Our project represents an ambitious attempt to bridge the gap between qualitative online discourse and quantitative market outcomes. Though we haven't yet discovered a reliable predictive relationship, our exploration has yielded valuable insights into both the technical challenges of sentiment analysis and the complex dynamics of cryptocurrency markets.&lt;/p&gt;
&lt;p&gt;We welcome feedback and suggestions as we continue to refine our approach and explore this fascinating intersection of technology and finance.&lt;/p&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;p&gt;Forbes. (2022, September 20). What Really Happened To LUNA Crypto? Forbes. &lt;a href="https://www.forbes.com/sites/qai/2022/09/20/what-really-happened-to-luna-crypto/"&gt;https://www.forbes.com/sites/qai/2022/09/20/what-really-happened-to-luna-crypto/&lt;/a&gt;  &lt;/p&gt;
&lt;p&gt;Lee, S., Lee, J., &amp;amp; Lee, Y. (2022). Dissecting the Terra-LUNA crash: Evidence from the spillover effect and information flow. Finance Research Letters, 53(1544-6123). &lt;a href="https://doi.org/10.1016/j.frl.2022.103590"&gt;https://doi.org/10.1016/j.frl.2022.103590&lt;/a&gt;&lt;/p&gt;</content><category term="Reflective Report"></category><category term="Group DeepText Analysts"></category></entry><entry><title>Diving Deep - First Reflections on Journey from the DepthSeeker</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/diving-deep-first-reflections-on-journey-from-the-depthseeker.html" rel="alternate"></link><published>2025-03-24T23:30:00+08:00</published><updated>2025-03-24T23:30:00+08:00</updated><author><name>FINA4350 Students 2025</name></author><id>tag:buehlmaier.github.io,2025-03-24:/FINA4350-student-blog-2025-01/diving-deep-first-reflections-on-journey-from-the-depthseeker.html</id><summary type="html">&lt;h1&gt;&lt;em&gt;First Reflection  - Depthseeker&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;Welcome to the DepthSeeker blog, where we document our journey exploring the correlation between social media discussion patterns and cryptocurrency price dynamics.&lt;/p&gt;
&lt;h1&gt;&lt;em&gt;Team Introduction&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;Our interdisciplinary team brings diverse perspectives to this project: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Anson (Data Science) - Leading our data cleaning and preprocessing, wherein he could utilize his …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h1&gt;&lt;em&gt;First Reflection  - Depthseeker&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;Welcome to the DepthSeeker blog, where we document our journey exploring the correlation between social media discussion patterns and cryptocurrency price dynamics.&lt;/p&gt;
&lt;h1&gt;&lt;em&gt;Team Introduction&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;Our interdisciplinary team brings diverse perspectives to this project: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Anson (Data Science) - Leading our data cleaning and preprocessing, wherein he could utilize his ability to deal with structured data&lt;/li&gt;
&lt;li&gt;Barbie (FinTech) - Helping with the preprocessing of data and taking responsibility for report writing based on her experience in fintech&lt;/li&gt;
&lt;li&gt;Woody (Computer Science) - Heading our web scraping activities and handling data visualization, leveraging his programming expertise&lt;/li&gt;
&lt;li&gt;Apollo (Economics &amp;amp; Finance) - Providing financial analysis expertise and contributing to sentiment analysis, drawing on his economic knowledge&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This blend of backgrounds allows us to approach our research from different angles, bringing technical know-how with financial insight. For this particular blog entry, Woody shared his experience with the technical side of data collection, while Barbie helped shape these experiences into a cohesive narrative.&lt;/p&gt;
&lt;h1&gt;&lt;em&gt;Our Project Motivation: Beyond Trading Strategies&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;While our first presentation highlighted several practical market applications for our research, including sentiment-driven trading strategies and risk management tools, our ambitions extend far beyond these applications. The group name, "DepthSeeker," captures our true purpose: to dive into the noisy surface of crypto markets and uncover the hidden patterns connecting social sentiment and price movements.&lt;/p&gt;
&lt;p&gt;As students with diverse academic backgrounds, we are confronted with the challenge of trying to glean valuable signals from the noisy and sometimes conflicting crypto community. We are thrilled at the prospect of working at the intersection of computer science, behavioral finance, and financial mathematics and discovering areas outside of our respective fields of expertise. In addition to acquiring technical NLP and data analysis abilities, we hope to make academic contributions to cryptocurrency market dynamics research. This project is our learning laboratory where theoretical knowledge meets real-world data problems.&lt;/p&gt;
&lt;h1&gt;&lt;em&gt;The Unexpected Complexity of Web Scraping&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;When we first outlined our project timeline, Woody volunteered to carry out the data collection part, which was scraping Reddit and Discord for crypto discussions. What initially seemed easy in theory quickly became a labyrinth of technical challenges.&lt;/p&gt;
&lt;h1&gt;&lt;em&gt;Discord: A Walled Garden&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;To perform discord scraping, we need to first find 3 elements&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;USER_TOKEN&lt;/em&gt;: [could be found in developer tools]&lt;/li&gt;
&lt;li&gt;&lt;em&gt;SERVER_ID&lt;/em&gt;: [after opening the developer mode in discord, right click the server icon]&lt;/li&gt;
&lt;li&gt;&lt;em&gt;CHANNEL_ID&lt;/em&gt;: [after opening the developer mode in discord, right click the channel icon]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These elements allow us to scrape a specific channel within a specific server, targeting messages before a specified time.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;async&lt;/span&gt; &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;scrape_history&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;before_time&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_batches&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Scrape historical messages in batches until done or limit reached.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
   &lt;span class="n"&gt;total_messages&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
   &lt;span class="n"&gt;batch_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

   &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;batch_count&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;max_batches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# Limit batches to avoid over-scraping&lt;/span&gt;
       &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Batch &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;batch_count&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;: Scraping before &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;before_time&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
       &lt;span class="n"&gt;messages_scraped&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
       &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;crypto_discord_raw.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;encoding&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;utf-8&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
           &lt;span class="k"&gt;async&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;message&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;channel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;history&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;before&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;before_time&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
               &lt;span class="n"&gt;messages_scraped&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
               &lt;span class="n"&gt;total_messages&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
               &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;message&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;created_at&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; | &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;message&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;author&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; | &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;message&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
               &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flush&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
               &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;messages_scraped&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                   &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Batch &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;batch_count&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;: Scraped &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;messages_scraped&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; messages (Total: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;total_messages&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
               &lt;span class="k"&gt;await&lt;/span&gt; &lt;span class="n"&gt;asyncio&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
           &lt;span class="c1"&gt;# Update before_time to the oldest message in this batch&lt;/span&gt;
           &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;messages_scraped&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
               &lt;span class="n"&gt;before_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;message&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;created_at&lt;/span&gt;
           &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
               &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;No more messages to scrape.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
               &lt;span class="k"&gt;break&lt;/span&gt;
       &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Batch &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;batch_count&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; complete. Scraped &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;messages_scraped&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; messages.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
       &lt;span class="n"&gt;batch_count&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
       &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;messages_scraped&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# Less than limit means we hit the start&lt;/span&gt;
           &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Reached the beginning of the channel history.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
           &lt;span class="k"&gt;break&lt;/span&gt;
       &lt;span class="k"&gt;await&lt;/span&gt; &lt;span class="n"&gt;asyncio&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# Pause between batches to avoid rate limits&lt;/span&gt;

   &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Scraping complete. Total messages scraped: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;total_messages&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After the initial setup, I quickly encountered an issue: the current version of the discord.py API is not user-friendly for scraping with a user account. Using a bot for scraping is generally preferred, but since we want to use a user account for simplicity, the latest version poses challenges. To address this, we can use version 1.7.3 of discord.py, which offers easier manipulation for Discord scraping with a user account.&lt;/p&gt;
&lt;p&gt;To install this specific version, run the following command in your terminal:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="n"&gt;discord&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mf"&gt;1.7.3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Woody also discovered a practical limitation with Discord scraping: each request retrieves only about 1,000 messages. This makes selecting a high-quality channel in a reputable server crucial for finding valuable cryptocurrency conversations. Many Discord servers are filled with casual chatter and off-topic noise, diluting crypto-related content. We are still on the hunt for more focused channels that host substantive crypto discussions.&lt;/p&gt;
&lt;h1&gt;&lt;em&gt;Reddit: API Challenge&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;To perform Reddit scraping, we first need to create an app within Reddit. This process allows us to obtain the client_id and client_secret, which are essential for initiating the scraping.
Once these credentials are secured, we can begin scraping data, such as the latest posts and comments, using the following approach in our code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Fetch the next batch of posts&lt;/span&gt;
       &lt;span class="n"&gt;submissions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;subreddit&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;before&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;last_id&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;last_id&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{})&lt;/span&gt;
       &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;submission&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;submissions&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
           &lt;span class="n"&gt;check_counter&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# Increment check counter for every post&lt;/span&gt;
           &lt;span class="n"&gt;current_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;created_utc&lt;/span&gt;

           &lt;span class="c1"&gt;# Print every 100th post being checked&lt;/span&gt;
           &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;check_counter&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
               &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Checking post &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromtimestamp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

           &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;current_time&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;end_date&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# Skip posts after Mar 24, 2025&lt;/span&gt;
               &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;  -&amp;gt; Skipping (after end_date)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
               &lt;span class="k"&gt;continue&lt;/span&gt;
           &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;current_time&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;start_date&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# Stop if before Mar 14, 2025&lt;/span&gt;
               &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;  -&amp;gt; Stopping (before start_date)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
               &lt;span class="k"&gt;break&lt;/span&gt;

           &lt;span class="n"&gt;post_count&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
           &lt;span class="n"&gt;total_posts&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
           &lt;span class="n"&gt;process_counter&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;  &lt;span class="c1"&gt;# Increment process counter for every processed post&lt;/span&gt;
           &lt;span class="n"&gt;batch_posts&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

           &lt;span class="c1"&gt;# Print every 100th processed post&lt;/span&gt;
           &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;process_counter&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
               &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Processing post #&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;total_posts&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;: &amp;#39;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;#39; (ID: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;, Time: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromtimestamp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

           &lt;span class="c1"&gt;# Store post data&lt;/span&gt;
           &lt;span class="n"&gt;posts_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
               &lt;span class="s1"&gt;&amp;#39;post_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="s1"&gt;&amp;#39;title&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="s1"&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;selftext&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="s1"&gt;&amp;#39;created_utc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromtimestamp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_time&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
               &lt;span class="s1"&gt;&amp;#39;score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="s1"&gt;&amp;#39;num_comments&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_comments&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
               &lt;span class="s1"&gt;&amp;#39;url&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;
           &lt;span class="p"&gt;})&lt;/span&gt;

           &lt;span class="c1"&gt;# Fetch all comments&lt;/span&gt;
           &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
               &lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;comments&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace_more&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
               &lt;span class="n"&gt;comment_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
               &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;comment&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;comments&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;list&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
                   &lt;span class="n"&gt;comment_count&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
                   &lt;span class="n"&gt;comments_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
                       &lt;span class="s1"&gt;&amp;#39;post_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                       &lt;span class="s1"&gt;&amp;#39;comment_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;comment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                       &lt;span class="s1"&gt;&amp;#39;body&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;comment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;body&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                       &lt;span class="s1"&gt;&amp;#39;created_utc&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromtimestamp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;comment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;created_utc&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                       &lt;span class="s1"&gt;&amp;#39;score&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;comment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                       &lt;span class="s1"&gt;&amp;#39;parent_id&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;comment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent_id&lt;/span&gt;
                   &lt;span class="p"&gt;})&lt;/span&gt;
               &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;  -&amp;gt; Fetched &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;comment_count&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; comments for post &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
           &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;Exception&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
               &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;  -&amp;gt; Error fetching comments for post &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;submission&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

           &lt;span class="n"&gt;time&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sleep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Respect rate limits&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Our Reddit data acquisition method hit an unexpected roadblock when Woody uncovered major API changes introduced by Reddit in 2023. Specifically, PRAW (Python Reddit API Wrapper) no longer supports retrieving posts between two specific dates—a feature removed starting with version 6.0.0. Consequently, our current scraping approach is restricted to fetching only the latest posts, as the official API’s free tier no longer supports time-based search queries.
To address this limitation, we’re taking the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We are actively exploring alternative methods to access historical Reddit data from specific time periods, which would enhance our historical analysis capabilities.&lt;/li&gt;
&lt;li&gt;In parallel, we are building our own cryptocurrency price dataset, ensuring the dates align with our text data for more cohesive analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;&lt;em&gt;Cryptocurrency Price Data&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;After reviewing datasets available on Kaggle and Hugging Face, Woody determined that most were too outdated for our needs. To overcome this, we turned to direct API calls to Coinbase, which provide historical data for any cryptocurrency we’re interested in, across any timeframe. This approach also opens the door to incorporating real-time pricing into future project implementations.&lt;/p&gt;
&lt;p&gt;For example, we can use the following API call to retrieve spot prices with a historical date parameter:
response = requests.get(f'https://api.coinbase.com/v2/prices/{coin_pair}/spot?date={date}')&lt;/p&gt;
&lt;p&gt;Note that only the spot price endpoint supports the date parameter for historical price requests.&lt;/p&gt;
&lt;p&gt;To illustrate, if we want to fetch BTC/USD pricing data from one year ago, we can structure the request like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;csv&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;datetime&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;timedelta&lt;/span&gt;


&lt;span class="c1"&gt;# Define the start and end dates for the previous year&lt;/span&gt;
&lt;span class="n"&gt;end_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;month&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;day&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;31&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;start_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;end_date&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;end_date&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="c1"&gt;# Define the list of dates to fetch&lt;/span&gt;
&lt;span class="n"&gt;date_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;start_date&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;timedelta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strftime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;%Y-%m-&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;end_date&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start_date&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;coin_pair&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;BTC-USD&amp;#39;&lt;/span&gt;
&lt;span class="c1"&gt;# Prepare CSV file&lt;/span&gt;
&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;btc_historical_data.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;w&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;newline&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
   &lt;span class="n"&gt;writer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="c1"&gt;# Write header&lt;/span&gt;
   &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writerow&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Amount&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Base&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Currency&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

   &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;date&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;date_list&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
       &lt;span class="c1"&gt;# Make API request for each date&lt;/span&gt;
       &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;https://api.coinbase.com/v2/prices/&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;coin_pair&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;/spot?date=&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
       &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


       &lt;span class="c1"&gt;# Extract necessary information&lt;/span&gt;
       &lt;span class="n"&gt;amount&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;amount&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
       &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writerow&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;amount&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;BTC&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;USD&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;


&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;CSV file has been created successfully.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1&gt;&lt;em&gt;Lesson Learnt&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;We have already learned several valuable lessons during this initial stage of the project. What began as a straightforward data collection task evolved into a difficult engineering challenge that considerably pushed our technical abilities forward. We discovered that API documentation literacy is essential, as a thorough reading of platform documentation before implementation would have spared countless hours of debugging and redevelopment. Our Discord experience taught us that specific library versions can distinguish between success and failure, highlighting the importance of dependency management.&lt;/p&gt;
&lt;p&gt;We also learned to maximize value from limited computational and financial resources. Strategic data sampling from authoritative sources yielded better results than processing large volumes of low-quality content. These resource optimization techniques proved essential, given our academic project constraints.&lt;/p&gt;
&lt;p&gt;These challenges ultimately strengthened our research methodology. Small-scale pilot testing revealed limitations in our approach before we committed further resources to potentially flawed strategies, allowing us to course-correct early and build a more robust analytical framework.&lt;/p&gt;
&lt;h1&gt;&lt;em&gt;Looking Forward&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;As we move into the next phase of our project, we will focus on identifying and integrating higher-quality data sources while implementing rigorous data cleansing processes. Our preprocessing pipeline will standardize the dataset by removing irrelevant characters, punctuation, and stop words. We foresee it as particularly challenging owing to the high noise inherent in social media text data. This racket, made up of emojis, platform-specific slang, and intentional misspellings, presents obstacles to correct sentiment analysis. Nevertheless, we recognize that comprehensive cleaning and preprocessing are not merely technical necessities but fundamental prerequisites for coherent signal extraction from the untamed landscape of cryptocurrency discussion online.&lt;/p&gt;
&lt;p&gt;Stay tuned for our next blog post, where we will share our experience working on the project!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;(Word Count: 1140)&lt;/em&gt;&lt;/p&gt;</content><category term="Reflective Report"></category><category term="Group Depthseeker"></category></entry><entry><title>Data Preprocessing for Sentiment Analysis in Earning Call Transcripts (by Group "Four Guys")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/data-preprocessing-for-sentiment-analysis-in-earning-call-transcripts-by-group-four-guys.html" rel="alternate"></link><published>2025-03-24T20:00:00+08:00</published><updated>2025-03-24T20:00:00+08:00</updated><author><name>FINA4350 Students 2025</name></author><id>tag:buehlmaier.github.io,2025-03-24:/FINA4350-student-blog-2025-01/data-preprocessing-for-sentiment-analysis-in-earning-call-transcripts-by-group-four-guys.html</id><summary type="html">&lt;p&gt;By Group "Four Guys"&lt;/p&gt;
&lt;h2&gt;Exploring the power of sentiments in earning calls&lt;/h2&gt;
&lt;p&gt;In the fast-paced world of finance, earnings calls serve as a critical bridge between companies and their stakeholders. In these meetings, company executives discuss financial results, strategic priorities, and future outlooks, which generate a wealth of unstructured textual …&lt;/p&gt;</summary><content type="html">&lt;p&gt;By Group "Four Guys"&lt;/p&gt;
&lt;h2&gt;Exploring the power of sentiments in earning calls&lt;/h2&gt;
&lt;p&gt;In the fast-paced world of finance, earnings calls serve as a critical bridge between companies and their stakeholders. In these meetings, company executives discuss financial results, strategic priorities, and future outlooks, which generate a wealth of unstructured textual data. It is not just about reporting numbers like revenue and profits, they also craft narratives with words that project confidence or on the other hand, show hesitations and risk. Hence, our group is interested in finding out possible relationships between these unspoken cues and its stock price movements.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Picture showing Vint" src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/images/Four-Guys_01_Vint.png"&gt;&lt;/p&gt;
&lt;p&gt;With advancements in natural language processing, we can now quantify this sentiment and explore its tangible impact on financial markets.&lt;/p&gt;
&lt;p&gt;By analyzing management’s tone (positive, negative, or neutral) and correlating it with post-call stock returns, we aim to answer two key questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Does a positive sentiment in earnings calls correlate with stock price increases, and does negative sentiment precede declines?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can sentiment analysis serve as a reliable predictive tool for investor decision-making?&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In order to test our hypothesis, we will have to explore and gather these data:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Earnings call transcripts: from S&amp;amp;P 500 companies over the past years &lt;/li&gt;
&lt;li&gt;Stock price data: Daily closing prices for the same companies, specifically focusing on 1-day and 1-week post-call windows to capture short-term market reactions.&lt;/li&gt;
&lt;li&gt;Alignment: Precise pairing of transcripts sentiments with their corresponding stock return calculations to ensure accurate analysis.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To initiate our project, the first step is undoubtedly data collection. This step is critical in the data workflow, as the quality of our data directly influences all subsequent processes. If we begin with subpar input, the output is unlikely to be much better. In addition to data quality, we must also consider the cost of obtaining information.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Garbage in, Garbage out” ~ George Fuechsel, an IBM programmer&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For our data mining efforts, we require a substantial number of earnings call transcripts from various companies and different time periods. Ideally, we are seeking a freely accessible database with API available. However, our research has indicated that most high-quality databases come with significant subscription fees, which adds to the overall cost of information acquisition, as shown in the following table.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Table of database" src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/images/Four-Guys_01_Table.png"&gt;&lt;/p&gt;
&lt;p&gt;For some platforms, such as FactSet, while free trials may be available, they typically have limitations, either in duration or by requiring detailed company information to access.&lt;/p&gt;
&lt;p&gt;We have discovered a free database provided by the Motley Fool that offers earnings call transcripts. However, there is no official API available. We have decided to develop our own scraper to efficiently download a large number of earnings call transcripts. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Earning Transcripts from Motley Fool" src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/images/Four-Guys_01_Earnings.png"&gt;&lt;/p&gt;
&lt;h2&gt;1. Scraping Links to Transcripts&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tqdm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;page&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;https://www.fool.com/earnings-call-transcripts/filtered_articles_by_page/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cookies&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;cookies&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;headers&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;soup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;html&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;html.parser&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;links&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;find_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;class_&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;flex-shrink-0 w-1/3 mr-16px sm:w-auto&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extend&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://www.fool.com&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;link&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;href&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;link&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;links&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We used BeautifulSoup and python Requests to scrap and stores 500 webpages of links that points to past transcripts from the Motley Fool’s archive. Each page is then stored in Redis for caching. 
We are using pandarallel for parallel processing in later parts to scrap transcripts from each link efficiently, and Redis is initialized for our data pipeline. &lt;/p&gt;
&lt;h2&gt;2. Get list of S&amp;amp;P 500 tickers, extract price data and calculate stock returns&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;yf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;download&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stock_code&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;s_p&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Symbol&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_list&lt;/span&gt;&lt;span class="p"&gt;()],&lt;/span&gt; &lt;span class="n"&gt;period&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;4y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_stock_returns&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
   &lt;span class="n"&gt;start_loc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
   &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stock_code&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
       &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
   &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
       &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
           &lt;span class="n"&gt;start_loc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_loc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
       &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
           &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
               &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timedelta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
               &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                   &lt;span class="n"&gt;start_loc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_loc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

       &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;start_loc&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
           &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inf&lt;/span&gt;
       &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;start_loc&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;start_loc&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;row&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stock_code&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
       &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
           &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inf&lt;/span&gt;
       &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After extracting the transcript, stock ticker and date, we merge them into a dataframe. We then extract a list of S&amp;amp;P 500 companies and extract their price data from Yahoo Finance by bulk downloading all stock codes into the dataframe using only the closing price. The get_stock_returns function takes each transcript (row), then obtain the prices of that stock in the next few days and obtain its 1-to-7 day returns. If one of the day is not a trading day, then it shifts onto the next trading day. &lt;/p&gt;
&lt;h2&gt;3. Sentiment Analysis&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;analyzer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SentimentIntensityAnalyzer&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# Define a function to compute sentiment score from text.&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;compute_sentiment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# If text is NaN, return np.nan&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
    &lt;span class="n"&gt;sentiment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;analyzer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polarity_scores&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# Use the compound score which summarizes overall sentiment.&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;sentiment&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;compound&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Next, we will calculate the correlations between the sentiment score and the return columns.&lt;/span&gt;
&lt;span class="c1"&gt;# List the columns that represent returns:&lt;/span&gt;
&lt;span class="n"&gt;return_columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;_day_return&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="c1"&gt;# We&amp;#39;ll add &amp;#39;sentiment&amp;#39; to the list for correlation analysis.&lt;/span&gt;
&lt;span class="n"&gt;columns_of_interest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sentiment&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;return_columns&lt;/span&gt;

&lt;span class="c1"&gt;# Apply sentiment analysis on the transcript text column&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;sentiment&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cleaned_data&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parallel_apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;TextBlob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sentiment&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polarity&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Calculate the correlation matrix for these columns.&lt;/span&gt;
&lt;span class="n"&gt;corr_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;columns_of_interest&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;corr&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After establishing the data pipeline (collecting, cleaning and pre-proecssing), we now start to analyse sentiment based on the earning calls transcript text and summarizing the overall sentiment using the pre-trained sentiment analysis model from the Vader module. After which we calculate the correlations between sentiment score and the stock reutrn columns (1-to-7 days) and visualize using a correlation matrix and heatmap. The heatmap which can be seen below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Heatmap Diagram" src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/images/Four-Guys_01_Heatmap.png"&gt;
&lt;img alt="7 day Scatterplot Diagram" src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/images/Four-Guys_01_Scatter.png"&gt;&lt;/p&gt;
&lt;p&gt;The scatterplot and the correlation heatmap suggests a low correlation between returns and the sentiment score calculated by this method. We will explore other analyzers in thhe future and see if more information can extracted.&lt;/p&gt;</content><category term="Reflective Report"></category><category term="Group Four Guys"></category></entry><entry><title>Tesla Sales Decoded: A Journey Through Data-Driven Narratives (by Group "FinBlazers")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/tesla-sales-decoded-a-journey-through-data-driven-narratives-by-group-finblazers.html" rel="alternate"></link><published>2025-03-24T17:00:00+08:00</published><updated>2025-03-24T17:00:00+08:00</updated><author><name>FINA4350 Students 2025</name></author><id>tag:buehlmaier.github.io,2025-03-24:/FINA4350-student-blog-2025-01/tesla-sales-decoded-a-journey-through-data-driven-narratives-by-group-finblazers.html</id><summary type="html">&lt;h2&gt;Group Members&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Angus Fung&lt;/strong&gt;&lt;br&gt;
  Majoring in Financial Technology with minors in Finance and Computer Science. Passionate about entrepreneurship, business strategy, and emerging tech. Loves problem-solving, exploring new ideas, and playing basketball and table tennis.  &lt;a href="https://github.com/angusf777"&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Haoze Lu (Peter)&lt;/strong&gt;&lt;br&gt;
  Majoring in Applied AI with minors in Finance and Computer Science. Passionate …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h2&gt;Group Members&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Angus Fung&lt;/strong&gt;&lt;br&gt;
  Majoring in Financial Technology with minors in Finance and Computer Science. Passionate about entrepreneurship, business strategy, and emerging tech. Loves problem-solving, exploring new ideas, and playing basketball and table tennis.  &lt;a href="https://github.com/angusf777"&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Haoze Lu (Peter)&lt;/strong&gt;&lt;br&gt;
  Majoring in Applied AI with minors in Finance and Computer Science. Passionate about AI technology, financial technology, and business strategy analysis. Loves travelling, badminton, and swimming. Always exploring.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Leung Cheuk Yiu (Janice)&lt;/strong&gt;&lt;br&gt;
  Majoring in Financial Technology with a keen interest in digital experience transformation, digital strategy, and business analysis. Passionate about utilizing technology to enhance user experiences and drive business success.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hui Jing Tung (Bernice)&lt;/strong&gt;&lt;br&gt;
  Majoring in Financial Technology. Interested in leveraging technology in the financial world, particularly in streamlining processes and driving product insights in the banking field.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Qian Yongkun Jonathan&lt;/strong&gt;
  Majoring in Financial Technology with minors in Finance and Computer Science. Passionate about the integration of cutting-edge technology in the finance industry.  Loves playing football and music.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Spark: Igniting Our Journey to Decode Tesla's Sales Narrative&lt;/h2&gt;
&lt;p&gt;In this blog post, we will illustrate how the dynamics of Tesla’s market sparked our idea to predict Tesla EV sales using natural language processing (NLP). We’ll cover our journey from the initial concept to the concrete steps we’re taking. Along the way, we’ll outline our motivations, discuss the challenges we anticipate, and describe the approach we’ve adopted to refine our methods and address potential obstacles.&lt;/p&gt;
&lt;p&gt;Our goal is to provide a transparent overview of our process, highlighting the ways in which we have tested and validated our hypotheses. By continuously updating our progress, we hope to offer insights into our problem-solving journey, showcase our team’s dynamic collaboration, and ultimately demonstrate how qualitative narratives can enhance quantitative forecasting. Stay tuned as we share our milestones, learning experiences, and ongoing refinements to achieve our end goals.&lt;/p&gt;
&lt;h2&gt;Tesla's Odyssey: From Revolutionary Beginnings to Turbulent Times&lt;/h2&gt;
&lt;p&gt;Our idea originated from some spirited conversations among our team, and it became crystal clear that Tesla's story is about so much more than just the numbers on a balance sheet. It is about the compelling narratives that have shaped its path.&lt;/p&gt;
&lt;p&gt;Since its founding, Tesla has revolutionized the automotive and energy sectors, shaking up traditional norms with its groundbreaking technology and bold vision. However, this journey has had its fair share of bumps along the way. From fierce market competition to production challenges, Tesla has navigated a landscape filled with obstacles. &lt;/p&gt;
&lt;p&gt;Today, Tesla faces increasing pressure from domestic EV manufacturers, particularly in China, its most important market. In 2024 alone, Tesla sold over 650,000 cars in China, yet its market share is shrinking. The competition has intensified, with companies like BYD aggressively pricing their vehicles, leading to a price war that is eroding Tesla’s profit margins. In January 2025, Tesla’s sales in China dropped 11.5% year-over-year, while BYD surged 47% in the same period.&lt;/p&gt;
&lt;p&gt;The challenges extend beyond China. In Europe, Tesla’s sales declined 45% year-over-year in January 2025, even as overall EV adoption in the region continued to grow. Contributing factors include rising competition, the reduction of government subsidies, and—perhaps most controversially—Elon Musk’s political activism, which has influenced public sentiment.&lt;/p&gt;
&lt;p&gt;Financially, Tesla is also seeing signs of strain. While it once boasted industry-leading profit margins, recent price cuts to maintain competitiveness have weighed heavily on profitability. In Q4 2024, Tesla’s revenue grew by only 2%, while operating income fell 23% due to rising costs. These pressures highlight the need for a deeper analysis of how external narratives—ranging from competition to investor sentiment—impact Tesla’s EV sales trajectory.&lt;/p&gt;
&lt;p&gt;Yet, through all these turbulent times, Tesla's commitment to innovation and sustainability continues to inspire. Despite financial pressures and market challenges, Tesla remains at the forefront of technological advancement, for example, it has been investing heavily in artificial intelligence and autonomous driving. The company is training its Full Self-Driving (FSD) software with billions of miles of real-world driving data, pushing the boundaries of AI-driven autonomy. While its AI-driven autonomy ambitions are promising, the monetization timeline remains uncertain. Investors are awaiting concrete progress on Tesla's Robotaxi initiative, which could be a game-changer to sales if successful but remains speculative at this stage.&lt;/p&gt;
&lt;p&gt;Our project aims to capture these multifaceted narratives by utilizing NLP to analyze how factors such as the launch of a new model, competitive pressures, and shifts in investor sentiment impact Tesla’s electric vehicle (EV) sales. By doing so, we hope to bridge the gap between qualitative narratives and quantitative performance, providing a fresh perspective on how revolutionary ideas can both drive and challenge market success.&lt;/p&gt;
&lt;h2&gt;A Tiered Approach? Why?&lt;/h2&gt;
&lt;p&gt;Taking on a project like predicting Tesla EV sales with natural language processing (NLP) can be quite daunting. The vast amount of data, complexities of modern NLP techniques, and the rapidly evolving market conditions can be easily overwhelming for us. We understand that if we jump headfirst into creating a fully integrated deep-learning model right away, we could face significant challenges. For example, we might encounter delays in data collection, overfitting or convergence issues, and unexpected gaps in our data could affect our predictions. Trying to manage all these factors simultaneously could jeopardize our ambitious goals. &lt;/p&gt;
&lt;h2&gt;Expected Challenges and Possible Failures&lt;/h2&gt;
&lt;p&gt;First of all, the data quality and availability present significant challenges for our project. Inconsistent or incomplete datasets from sources like social media, news, and financial filings might hinder our model training efforts, making it difficult to draw accurate insights. Moreover, the complexity of NLP models introduces additional difficulties; advanced techniques such as transformers and deep topic modeling may lead to overfitting if not managed carefully. In addition to these technical challenges, we must also consider temporal and contextual variability. The factors driving Tesla's sales can shift rapidly due to sudden policy changes, competitive moves, or fluctuations in public sentiment, which makes it tough to maintain stable model performance over time. Furthermore, as students, we frequently face resource constraints, including limited computational power and time. This complicates our ability to handle large-scale data processing or conduct deep learning experiments effectively. These factors create a challenging landscape for our analysis, requiring careful planning and strategic approaches to overcome.&lt;/p&gt;
&lt;h2&gt;Breaking It Down Into Tiers&lt;/h2&gt;
&lt;p&gt;We’ve structured our approach into clear, incremental tiers to mitigate these risks and ensure steady progress. This tiered framework allows us to validate foundational ideas in Tier 1 before scaling up to more sophisticated Tier 2 and 3 methods. Should we encounter insurmountable challenges—be it data quality issues, model complexity, or time constraints—our early wins (such as basic sentiment analysis) will still provide valuable insights, ensuring that we don’t walk away empty-handed. This incremental methodology also offers natural checkpoints for refining our approach, adjusting scope, and re-aligning with data and resource availability realities. Ultimately, the tiered strategy helps us stay agile, systematically building toward our most ambitious goals without risking complete project failure if one element proves too difficult.&lt;/p&gt;
&lt;h3&gt;Tier 1 – Foundational Proof-of-Concept&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Establish a baseline correlation between simple sentiment metrics and Tesla sales.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data:&lt;/strong&gt; Focus on a single source (e.g., news headlines) plus Elon Musk's Tesla-related tweets.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Methods:&lt;/strong&gt; Basic sentiment analysis (VADER or TextBlob), followed by simple regression to measure the relationship with monthly or quarterly sales data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Tier 2 – Enhanced Multi-Source Analysis&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Integrate additional textual inputs and employ more sophisticated NLP techniques to enrich our predictive features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data:&lt;/strong&gt; Multiple news outlets, Reddit forums, Tesla’s financial filings, and potentially competitor announcements.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Methods:&lt;/strong&gt; BERT-based sentiment analysis, named entity recognition (NER) for key events, and topic modeling (LDA) to capture broader themes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Tier 3 – Advanced Thematic Exploration&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Goal:&lt;/strong&gt; Conduct deep thematic analysis to identify which narratives (e.g., competition, political controversies, new model releases) most strongly correlate with Tesla's sales.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data:&lt;/strong&gt; All previous sources plus macroeconomic indicators, government policy documents, and further competitor data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Methods:&lt;/strong&gt; Apply advanced LDA or other topic modelling techniques to quantify thematic trends and correlate them with sales, exploring time-lagged effects to better understand causal relationships.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data Sources&lt;/h2&gt;
&lt;p&gt;So far, we have identified key datasets to support our analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tesla Sales Data &amp;amp; Reports:&lt;/strong&gt;&lt;br&gt;
  Detailed monthly sales data for Tesla and other major car manufacturers from 2015 to 2024.&lt;br&gt;
&lt;a href="https://www.goodcarbadcar.net/brands/tesla-sales-data-reports/"&gt;Tesla Sales Data &amp;amp; Reports | GCBC&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Twitter Dataset:&lt;/strong&gt;&lt;br&gt;
  10,000 Twitter posts about Tesla, with detailed post dates up to Dec 7, 2022.&lt;br&gt;
&lt;a href="https://huggingface.co/datasets/hugginglearners/twitter-dataset-tesla"&gt;hugginglearners/twitter-dataset-tesla on Hugging Face&lt;/a&gt;&lt;br&gt;
  The code we use is as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  curl -X GET &amp;quot;https://datasets-server.huggingface.co/rows?dataset=hugginglearners%2Ftwitter-dataset-tesla&amp;amp;config=default&amp;amp;split=train&amp;amp;offset=0&amp;amp;length=100&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Financial News Articles (2020-2024):&lt;/strong&gt;
  Financial news about Tesla, primarily without specific dates.
  &lt;a href="https://www.kaggle.com/datasets/journeyyouyeonkim/microsoft-tesla-finance-news-articles2020-2024"&gt;Microsoft Tesla Finance News Articles (2020-2024) on Kaggle&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Financial News on Tesla and Elon Musk (2022):&lt;/strong&gt;
  Articles focused on Tesla and Elon Musk in 2022. 
  &lt;a href="https://www.kaggle.com/datasets/saleepshrestha/newspapers"&gt;Newspapers Dataset on Kaggle&lt;/a&gt; &lt;br&gt;
  The code we use is as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;kagglehub&lt;/span&gt;&lt;span class="w"&gt;    &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# Download latest version   &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;kagglehub&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataset_download&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;saleepshrestha/newspapers&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Path to dataset files:&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Analysis Plan&lt;/h2&gt;
&lt;p&gt;Looking at all the datasets we have obtained at hand, our first step into the project may be making use of the temporal information in the Twitter posts, and picking that financial news with a clear date or is known to belong to 2022, to use Dec 2022 as a cutting point, and use the dataset 1-2 years before Dec 2022 to predict the sales of the following year (2023) and validate with the existing figures. Apart from that, we also have an accessible dataset about the sales of competitors of Tesla, the correlation of sales with competitors is an achievable next step. In the meantime, we will be continuing with collecting new datasets to have a better analysis in each Tier.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Through our tiered approach, we look forward to harnessing the power of NLP to dissect the intricate narratives that drive Tesla's sales. By breaking down our ambitious goal into manageable, incremental steps, we aim to progressively build a robust model that accurately captures and predicts market dynamics. We are currently acquiring data sources and preprocessing them to fit our analytical needs. Stay tuned for further updates and detailed insights in our next post.&lt;/p&gt;</content><category term="Reflective Report"></category><category term="Group FinBlazers"></category></entry><entry><title>Building a Crypto Sentiment Trading Model (by Group "AI16Z")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/building-a-crypto-sentiment-trading-model-by-group-ai16z.html" rel="alternate"></link><published>2025-03-23T16:00:00+08:00</published><updated>2025-03-23T16:00:00+08:00</updated><author><name>FINA4350 Students 2025</name></author><id>tag:buehlmaier.github.io,2025-03-23:/FINA4350-student-blog-2025-01/building-a-crypto-sentiment-trading-model-by-group-ai16z.html</id><summary type="html">&lt;p&gt;By Group "AI16Z"&lt;/p&gt;
&lt;h2&gt;How We Chose Our Topic&lt;/h2&gt;
&lt;p&gt;Sentiment analysis is quite a strong financial instrument because it allows traders and investors to evaluate market moods via the analysis of news, social media, and other information. Of course, our passion for trading and investing led us to chase sentiment analysis …&lt;/p&gt;</summary><content type="html">&lt;p&gt;By Group "AI16Z"&lt;/p&gt;
&lt;h2&gt;How We Chose Our Topic&lt;/h2&gt;
&lt;p&gt;Sentiment analysis is quite a strong financial instrument because it allows traders and investors to evaluate market moods via the analysis of news, social media, and other information. Of course, our passion for trading and investing led us to chase sentiment analysis as a way to dive into how markets behave. Noticing its potential, we then settled on where in the market it would be most effective.&lt;/p&gt;
&lt;p&gt;Unlike conventional financial markets that are driven by fundamental indicators such as economic news and company earnings, the cryptocurrency market is very much influenced by sentiment. Market movement is often influenced by such events as news reports, social media posts, and tweets from influential figures as well as discussion forums online. Because of the decentralized and speculative nature of crypto assets, price action is dictated by market sentiment.&lt;/p&gt;
&lt;p&gt;Such an extreme reliance on sentiment makes the cryptocurrency market a flawless setup to analyze sentiment. By using complex natural language processing (NLP) techniques, machine learning software, and statistical analysis, we can measure the sentiment in markets systematically and objectively in real time. It helps us pick out trends, foresee market trends, and trade smartly. Noticing these advantages, we have opted to apply sentiment analysis in the crypto market specifically, where it can be of valuable assistance in identifying investor sentiment and predicting price action.&lt;/p&gt;
&lt;h2&gt;Data Collection&lt;/h2&gt;
&lt;p&gt;The first task is identifying reliable sources containing text data related to Bitcoin. The sources should also include the hidden sentiment in the market. At first, we discussed using both news articles and social media posts, such as Twitter, to capture a more general view of market sentiment. However, after further research, we realized that there are several obstacles to using social media data, such as the difficulty of filtering bot-generated posts, and noises. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Bot noise&lt;/strong&gt; – 30% of tweets from trending crypto hashtags were spam.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Context ambiguity&lt;/strong&gt; – Phrases like “This coin is fire!” could mean success or disaster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, we decided to focus only on news articles, which are more structured and with credible sources, for sentiment analysis.&lt;/p&gt;
&lt;p&gt;After evaluating several news sources, we eliminated a bunch of news platforms that provide limited Bitcoin-related articles. We finally selected Coindesk and NewsAPI owing to their API accessibility and extensive coverage of cryptocurrency-related news. These data sources provide historical Bitcoin-related news articles, allowing us to do a time-series analysis which is important for predicting future prices of Bitcoin.&lt;/p&gt;
&lt;p&gt;Here's an example of how we retrieve Coindesk news articles via API:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;https://data-api.coindesk.com/news/v1/article/list&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;headers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Content-type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;application/json; charset=UTF-8&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;api_key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;lt;API_KEY&amp;gt;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;to_ts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;datetime&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;now&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timestamp&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;api_key&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;api_key&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;lang&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;EN&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;categories&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;BTC&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;to_ts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;to_ts&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;headers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;json_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Potential Models &amp;amp; Training&lt;/h2&gt;
&lt;p&gt;During the initial phase of our project, we debated between using a lexicon-based approach (such as VADER) and machine learning models for data analysis. Both models exhibit pros and cons. The lexicon-based method was quick and interpretable; however, it struggled with detecting sarcasm and context-specific sentiment in financial news. On the other hand, training machine learning models provided greater accuracy but required labelled datasets, which were difficult to obtain. To balance these trade-offs, we decided on a hybrid approach—using lexicons to establish a baseline sentiment score and refining these results with machine learning models trained on labelled financial news.&lt;/p&gt;
&lt;p&gt;Our current focus involves selecting the optimal machine learning model from a suite of candidates, including Logistic Regression, KNN, Naïve Bayes, QDA, and LDA. We plan to develop the Python code for these models first, test them over a minimum of 10 trading days, and select the one with the highest average accuracy. We aim to finalize the Python code for these models before April and begin testing the results with time-series market data. This integration will allow us to analyze how sentiment trends correlate with price movements or trading volumes. Our &lt;strong&gt;Blog 2&lt;/strong&gt; would document daily model performances for transparency, while the final presentation will prioritize a summary of overall accuracy and practical insights due to time constraints.&lt;/p&gt;
&lt;h2&gt;Potential Limitation of Our Method&lt;/h2&gt;
&lt;p&gt;Our hybrid approach encounters the project’s compressed timeline. While testing models over 10 trading days provides a snapshot of performance, this short window may not adequately account for broader economic cycles—such as bull or bear markets—that inherently influence sentiment patterns. As a result, sudden volatility, such as geopolitical events could affect our model. We aim to mitigate this by explicitly acknowledging the economic context of our testing period in &lt;strong&gt;Blog 2&lt;/strong&gt; and cautioning against overgeneralizing our findings to all market conditions.&lt;/p&gt;</content><category term="Reflective Report"></category><category term="Group AI16Z"></category></entry><entry><title>Blog 1 (by Group "NLPredict")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/blog-1-by-group-nlpredict.html" rel="alternate"></link><published>2025-03-21T18:00:00+08:00</published><updated>2025-03-21T18:00:00+08:00</updated><author><name>FINA4350 Students 2025</name></author><id>tag:buehlmaier.github.io,2025-03-21:/FINA4350-student-blog-2025-01/blog-1-by-group-nlpredict.html</id><summary type="html">&lt;p&gt;Introduction&lt;/p&gt;
&lt;p&gt;The following blog covers the developmental progress of NLPredict by March 21st 2025 and will explore the various checkpoints, difficulties and breakthroughs our team has made throughout the developmental process.&lt;/p&gt;
&lt;p&gt;Currently our team has finished planning and programming the early phase of NLPredict’s model. We have also planned …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Introduction&lt;/p&gt;
&lt;p&gt;The following blog covers the developmental progress of NLPredict by March 21st 2025 and will explore the various checkpoints, difficulties and breakthroughs our team has made throughout the developmental process.&lt;/p&gt;
&lt;p&gt;Currently our team has finished planning and programming the early phase of NLPredict’s model. We have also planned for certain goals we aim to reach by the end of the current phase of development, namely to have finished research into the current financial market as well as beginning analysis and testing different model architectures.&lt;/p&gt;
&lt;p&gt;Market Research&lt;/p&gt;
&lt;p&gt;Research into the market was conducted by Jason, we note that there have been previous cases of using NLP and Sentiment Analysis as supporting but not mainstream predictors of stock prices. Presently, we understand that NLPredict is not a unique product in the market, in truth, there are many competitors of NLPredict in this regard. Due to our lack of distinction from other products in the market, we are considering elements which could allow us to stand out. In order to create distinction, we considered doing so from either an analytical perspective, focusing on making more precise predictions, or from a creative approach, focusing more on the elements of NLPredict’s creation process. One of the proposed key elements to making our product different creatively is the reliance and emphasis on using NLP and Sentiment Analysis instead of other data types such as numerical data from stock prices.&lt;/p&gt;
&lt;p&gt;Currently, for simplicity, we are using Yahoo Finance and Finviz and are scraping the articles’ text data as shown below:&lt;/p&gt;
&lt;p&gt;def get_news(ticker, company_name, days=30):
    """Collect financial news articles for the specified company with improved error handling."""
    print(f"Collecting news for {company_name} ({ticker})")
    news_items = []&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;Track&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;success&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;each&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;source&lt;/span&gt;
&lt;span class="nt"&gt;source_success&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;yahoo&amp;#39;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;False,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;marketwatch&amp;#39;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;False,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;seekingalpha&amp;#39;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;False,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;finviz&amp;#39;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;False&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;1&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;Try&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;Yahoo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;Finance&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;improved&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;headers&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;selectors&lt;/span&gt;
&lt;span class="nt"&gt;try&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;url&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://finance.yahoo.com/quote/{ticker}/news&amp;quot;&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;headers&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;User-Agent&amp;#39;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;Mozilla/5.0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;(Windows&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;NT&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;10.0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;Win64&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;x64)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;AppleWebKit/537.36&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;(KHTML,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;like&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;Gecko)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;Chrome/91.0.4472.124&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;Safari/537.36&amp;#39;,&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;Accept&amp;#39;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;text/html,application/xhtml+xml,application/xml&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="err"&gt;q=0.9,image/webp,*/*&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="err"&gt;q=0.8&amp;#39;,&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;Accept-Language&amp;#39;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;en-US,en&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="err"&gt;q=0.9&amp;#39;,&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;Accept-Encoding&amp;#39;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;gzip,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;deflate,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;br&amp;#39;,&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;Connection&amp;#39;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;keep-alive&amp;#39;,&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;Upgrade-Insecure-Requests&amp;#39;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;1&amp;#39;,&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;Cache-Control&amp;#39;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;&amp;#39;max-age=0&amp;#39;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The above code was used in order to obtain data from Yahoo Finance. Currently we have multiple such implementations of the above 'get_news' functions on various news sources. However, we want to find a more efficient method of increasing the number of inputs without having to increase the amount of code drastically. &lt;/p&gt;
&lt;p&gt;Current Development&lt;/p&gt;
&lt;p&gt;The development of NLPredict has been going smoothly. Herbert has compiled the source code required for data extraction, data modelling, data visualisation and feature engineering. The source code has undergone a few tests but the results have been less than ideal, thus there is still a need for editing the code. Presently, the code is still very primitive and is not completely optimised for the consumer to understand or use especially on devices with less memory or computing power. This is within expectations as the initial plan amongst our team was to create the product first before making it explainable and widely accessible to the market.&lt;/p&gt;
&lt;p&gt;An example of such code can be found below. The following code shows the change in market sentiment, the level of which was obtained from our Sentiment Analysis, over time. This allows for a direct visual comparison with the Stock Market at the time. In the source code, we have made multiple such visualisations each with different functions.&lt;/p&gt;
&lt;p&gt;def visualize_results(data, model):
    """Visualize sentiment vs. returns and predictions."""
    print("Generating visualizations...")
    plt.figure(figsize=(15, 10))&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;# Plot 1: Sentiment Time Series
plt.subplot(2, 2, 1)
plt.plot(data.index, data[&amp;#39;Sentiment&amp;#39;], &amp;#39;b-&amp;#39;, label=&amp;#39;Sentiment&amp;#39;)
plt.xlabel(&amp;#39;Date&amp;#39;)
plt.ylabel(&amp;#39;Sentiment Score&amp;#39;)
plt.title(&amp;#39;Sentiment Score Over Time&amp;#39;)
plt.grid(True, linestyle=&amp;#39;--&amp;#39;, alpha=0.7)
plt.legend()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;During the testing of the model, it was also noted that some level of additional data cleaning was required as well as adjustment of the hyperparameters of the model. This will be our upcoming focus as we aim to be able to gain more accurate and reliable results from the model first before tuning it for the long term. We have also encountered problems with acquiring data from certain stocks which may also be a matter for data preprocessing. Additionally, the model still has a time lag when compared to the rate at which news enters the market, as such, some level of preliminary prediction may be necessary to account for this duration.&lt;/p&gt;
&lt;p&gt;Current Objectives&lt;/p&gt;
&lt;p&gt;Our team is now entering discussions about increasing the variety of data sources as well as looking for other data sources to supplement our Sentiment Analysis. We are also looking at different metrics to gauge the accuracy of our model’s predictions. Other considerations such as using numerical data in contandem with textual data have also been proposed. Currently, our stance on this matter is to avoid using numerical data if possible, but to include it as support if it aids with the model’s predictions.&lt;/p&gt;
&lt;p&gt;Presently, we have not made significant breakthroughs outside of progressing our main tasks as a group. We have had some difficulties in reaching our goals within the given time period, mainly due to the scope of our project. While we have discussed the approach and whether or not we would like to revise our scope and approach, we have ultimately decided against doing so in favour of creating a more robust product.&lt;/p&gt;
&lt;p&gt;To conclude, we would like to offer some insight on the projections our group has for the future of this project. Looking forward, aside from the various issues touched on previously, we are aiming to focusing on finding any anomalies as well as potential points of error which may affect the accuracy of our predictions from a consumer's perspective. In particular, we would like to find some stocks which may not be as predictable with Sentiment Analysis than other stocks. Moreover, we would like to create a more user-friendly aspect to our model, making it more accessible not only from a technical perspective but also from a convenience perspective.&lt;/p&gt;</content><category term="Reflective Report"></category><category term="Group NLPredict"></category></entry><entry><title>Tips on Data Collection via API (by Group "FinText")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/tips-on-data-collection-via-api-by-group-fintext.html" rel="alternate"></link><published>2025-03-20T00:00:00+08:00</published><updated>2025-03-20T00:00:00+08:00</updated><author><name>FINA4350 Students 2025</name></author><id>tag:buehlmaier.github.io,2025-03-20:/FINA4350-student-blog-2025-01/tips-on-data-collection-via-api-by-group-fintext.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Data retrieval is the process of extracting relevant textual information from large datasets or databases. It's a crucial first step in many NLP workflows. By harnessing the power of APIs (Application Programming Interfaces), you can request data on-demand to fuel various analytical endeavors.&lt;/p&gt;
&lt;p&gt;In this post, we'll explore fundamental …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Data retrieval is the process of extracting relevant textual information from large datasets or databases. It's a crucial first step in many NLP workflows. By harnessing the power of APIs (Application Programming Interfaces), you can request data on-demand to fuel various analytical endeavors.&lt;/p&gt;
&lt;p&gt;In this post, we'll explore fundamental strategies for collecting data through APIs. We'll also walk through sample code on data retrieval and discuss how can we handle large datasets.&lt;/p&gt;
&lt;h2&gt;Simple Data Retrieval using AРІ&lt;/h2&gt;
&lt;p&gt;If you're new to APIs, this simple demonstration can help you quickly understand data retrieval using API. Taking Alpha Vantage API as an example, we use Python's requests library to get news articles for Apple (AAPL).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt;
&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;https://www.alphavantage.co/query?function=NEWS_SENTIMENT&amp;amp;tickers=AAPL&amp;amp;apikey=demo&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Prints Data in a Nicely Formatted Way&lt;/h2&gt;
&lt;p&gt;Exploring your data using printed output is a great way to understand the structure before moving on to further analysis or storage. Python provides this handy pprint module for easier readability:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pprint&lt;/span&gt;
&lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PrettyPrinter&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;pp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pprint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This improves readability by neatly formatting nested data structures, making debugging and exploration simpler.&lt;/p&gt;
&lt;h2&gt;Dealing with Big Data&lt;/h2&gt;
&lt;p&gt;Many APIs allow you to fetch large datasets, but they often impose limits such as the response sizes. To deal with these constraints, you often need to make repeated calls, iterating through available pages or date ranges. Below is a sample pseudocode illustrating how you can handle paginated data collection:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Pseudocode

URL = &amp;quot;https://api.example.com/data&amp;quot;
API_KEY = &amp;quot;your_api_key_here&amp;quot;

data = []

loop through every date:
    loop through every page:
        response = request(URL, API_KEY, current_date, current_page)
        if response.status_code == 200:
            data.append(response.json())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When dealing with extensive datasets, waiting on each page or date sequentially can slow you down. You can speed up this process using asynchronous programming, which lets you request multiple pages or date ranges concurrently.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Pseudocode

async def fetch_data():
    do something
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Converting to a Pandas DataFrame&lt;/h2&gt;
&lt;p&gt;After collecting data from your API, one common next step is to convert that data into a Pandas DataFrame. Pandas is a powerful Python library for data cleaning, transformation and exploration.&lt;/p&gt;
&lt;p&gt;Assuming your API returns in JSON format, you can convert it using this code snippet:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Saving Data into Permanent Storage&lt;/h2&gt;
&lt;p&gt;After collecting your data, consider storing it in a permanent format for future analysis. Depending on your project requirements, you can save to in different formats such as Parquet, CSV, Excel:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="n"&gt;to_parquet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;filename.parquet&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Efficient columnar storage&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;filename.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Widely compatible format: comma-separated values&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_excel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;filename.xlsx&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# Excel spreadsheet&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you are ready to process the data, have fun with your NLP journey!&lt;/p&gt;</content><category term="Reflective Report"></category><category term="Group FinText"></category></entry><entry><title>Overcoming Sentiment Analysis Challenges with FinBERT and SpaCy (by Group "FinText")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/overcoming-sentiment-analysis-challenges-with-finbert-and-spacy-by-group-fintext.html" rel="alternate"></link><published>2025-03-14T00:00:00+08:00</published><updated>2025-03-14T00:00:00+08:00</updated><author><name>FINA4350 Students 2025</name></author><id>tag:buehlmaier.github.io,2025-03-14:/FINA4350-student-blog-2025-01/overcoming-sentiment-analysis-challenges-with-finbert-and-spacy-by-group-fintext.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Information is the foundation of decision-making in the fast-paced and ever-changing financial markets. From news articles and social media updates to earnings reports and analyst opinions, the constant stream of information provides investors with critical insights. However, the sheer volume and complexity of this information present significant challenges. How …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Information is the foundation of decision-making in the fast-paced and ever-changing financial markets. From news articles and social media updates to earnings reports and analyst opinions, the constant stream of information provides investors with critical insights. However, the sheer volume and complexity of this information present significant challenges. How can we extract actionable insights from this deluge of data? This question guided our group’s exploration of Natural Language Processing (NLP) techniques for financial text analysis.&lt;/p&gt;
&lt;p&gt;Our project, "News-driven NLP Quant," aims to analyze financial news to provide insights that inform trading strategies and optimize portfolio performance. As we worked on the project, we encountered several challenges that shaped our approach and led us to adopt domain-specific
models like FinBERT and feature extraction techniques such as Named-Entity Recognition (NER) using SpaCy.&lt;/p&gt;
&lt;h2&gt;The Challenge of TextBlob Sentiment Analysis in Financial Texts&lt;/h2&gt;
&lt;p&gt;Sentiment analysis emerged as a natural starting point for our project. By assessing the tone of financial news, we could quantify market sentiment and help investors make informed decisions. However, as we delved deeper, we realized that sentiment analysis in the financial domain is far from straightforward.&lt;/p&gt;
&lt;p&gt;Initially, we experimented with TextBlob, a popular Python library for sentiment analysis. While TextBlob is easy to use and provides a quick way to determine whether text has a positive, negative, or neutral tone, it has significant limitations when applied to financial texts. TextBlob
treats each word independently, lacking the ability to understand the contextual meaning of words. This is particularly problematic in financial texts, where domain-specific jargon and ambiguous phrases are common. Additionally, TextBlob’s general-purpose approach results in lower accuracy when analyzing the nuanced language of financial markets.&lt;/p&gt;
&lt;p&gt;Here’s an example of how we used TextBlob for sentiment analysis, which returns a polarity score based on the sentiment of the news article.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;textblob&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TextBlob&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TextBlob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;news_article&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polarity&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;While TextBlob provided a quick and easy way to gauge sentiment, its limitations led us to explore more advanced solutions.&lt;/p&gt;
&lt;h2&gt;Enhancing Sentiment Analysis with FinBERT&lt;/h2&gt;
&lt;p&gt;To address the shortcomings of traditional sentiment analysis tools, we turned to FinBERT, a pre-trained NLP model specifically designed for financial texts. FinBERT, available on Hugging Face, is fine-tuned on a large corpus of financial news and reports, enabling it to accurately interpret the nuanced language of financial markets.&lt;/p&gt;
&lt;p&gt;FinBERT excels at understanding context and domain-specific phrases, making it significantly more accurate than general-purpose models. For instance, terms like "closure" and "surge" can carry vastly different sentiments depending on the context. FinBERT can accurately analyze news headings such as "360 Energy Liability Management Accelerates Environmental Site Closure Business with Strategic Acquisition," recognizing it as a positive development. Similarly, it correctly interprets "Bitcoin's surge confuses even pro traders: 'It's trading like a Treasury'" as conveying a negative sentiment. By deciphering the nuanced meanings behind such terms, FinBERT delivers precise and actionable insights.&lt;/p&gt;
&lt;p&gt;Additionally, FinBERT’s context awareness allows it to handle complex linguistic features such as negation and sarcasm, which are often challenging for general-purpose tools. Its training on financial-specific language ensures that it captures the subtle nuances and jargon unique to the financial domain. This capability ensures that our sentiment analysis is both accurate and relevant to the financial domain.&lt;/p&gt;
&lt;p&gt;Here’s a code snippet demonstrating how we used FinBERT.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pipeline&lt;/span&gt;
&lt;span class="n"&gt;pipe&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pipeline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;text-classification&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;yiyanghkust/finbert-tone&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;pipe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;news_article&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let's consider this fictional news. The result shows that the news article input has a positive sentiment with 99.9% confidence. In contrast, TextBlob classified the same passage as neutral, failing to capture the positive sentiment evident in the article. This stark difference highlights FinBERT’s accuracy in analyzing financial news, providing more reliable and actionable insights.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fictional News March 18, 2025&lt;br&gt;
FinText Limited reported a 15% decline in revenue this quarter,
citing market volatility and slower enterprise adoption.
On the same day, CEO Alvin Ku announced the launch of Project NLP in FINA4350.
Despite the revenue drop, analysts at Morgan Stanley remain optimistic.
Investors are watching closely as FinText Limited pivots toward cutting-edge text analytics advancements to fuel future growth.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TextBlob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;news_article&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;polarity&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# TextBlob Output: Neutral (-0.0357)&lt;/span&gt;

&lt;span class="n"&gt;pipe&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;news_article&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# FinBERT Output: Strong Positive (0.9994)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Extracting Granular Insights with Named-Entity Recognition (NER)&lt;/h2&gt;
&lt;p&gt;While sentiment analysis provides a high-level view of market sentiment, it lacks granularity. To address this, we incorporated NER using SpaCy, an open-source NLP library with fast and accurate entity recognition capabilities.&lt;/p&gt;
&lt;p&gt;SpaCy’s deep-learning based recognition system is highly efficient, making it ideal for processing large volumes of financial text. It also allows for customization, enabling us to identify and classify domain-specific entities such as company names, stock tickers, and financial terms with high accuracy.&lt;/p&gt;
&lt;p&gt;Here’s an example of how we used SpaCy for NER. The model successfully identified and labelled entities such as person, date, and organization, as shown in the code snippet below. This capability is particularly useful for extracting structured information from unstructured financial texts, enabling us to analyze relationships between entities and gain deeper insights into market trends and events.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;spacy&lt;/span&gt;
&lt;span class="n"&gt;nlp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spacy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;en_core_web_lg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;doc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nlp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;news_article&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;spacy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;displacy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;render&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;doc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;style&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ent&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;jupyter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Picture showing the output of NER" src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/images/group-FinText-NER_spacy.png"&gt;&lt;/p&gt;
&lt;p&gt;By combining FinBERT with SpaCy’s NER capabilities, we linked sentiment scores to specific entities mentioned in financial news. For example, rather than assigning a general sentiment score to an article discussing multiple companies, our system identifies which companies are being discussed and associates each with its respective sentiment. This entity-specific approach ensures that investors can focus on the news most relevant to their portfolios.&lt;/p&gt;
&lt;h2&gt;Conclusion: A Continous Exploration of Financial NLP&lt;/h2&gt;
&lt;p&gt;Our project highlights the power of combining sentiment analysis, NER, and domain-specific models like FinBERT to tackle the challenges of financial NLP. By leveraging these advanced techniques, we can extract actionable insights from financial texts, enabling more informed decision-making in the markets. As we move forward, we will continue exploring advanced methods for NLP in financial markets to refine our approach.&lt;/p&gt;</content><category term="Reflective Report"></category><category term="Group FinText"></category></entry><entry><title>Demo Blog Post (by Group "Fintech Disruption")</title><link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/demo-blog-post-by-group-fintech-disruption.html" rel="alternate"></link><published>2025-01-17T01:12:00+08:00</published><updated>2025-01-17T01:12:00+08:00</updated><author><name>FINA4350 Students 2025</name></author><id>tag:buehlmaier.github.io,2025-01-17:/FINA4350-student-blog-2025-01/demo-blog-post-by-group-fintech-disruption.html</id><summary type="html">&lt;p&gt;By Group "Fintech Disruption"&lt;/p&gt;
&lt;p&gt;This is a demo blog post. Its purpose is to show how to use the basic
functionality of Markdown in the context of a blog post.&lt;/p&gt;
&lt;h2&gt;How to Include a Link and Python Code&lt;/h2&gt;
&lt;p&gt;We chose &lt;a href="http://www.investing.com"&gt;Investing.com&lt;/a&gt; to get the whole
year data of XRP …&lt;/p&gt;</summary><content type="html">&lt;p&gt;By Group "Fintech Disruption"&lt;/p&gt;
&lt;p&gt;This is a demo blog post. Its purpose is to show how to use the basic
functionality of Markdown in the context of a blog post.&lt;/p&gt;
&lt;h2&gt;How to Include a Link and Python Code&lt;/h2&gt;
&lt;p&gt;We chose &lt;a href="http://www.investing.com"&gt;Investing.com&lt;/a&gt; to get the whole
year data of XRP and recalculated the return and 30 days volatility.&lt;/p&gt;
&lt;p&gt;The code we use is as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;nltk&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;myvar&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt;
&lt;span class="n"&gt;DF&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;XRP-data.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;How to Include a Quote&lt;/h2&gt;
&lt;p&gt;As a famous hedge fund manager once said:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fed watching is a great tool to make money. I have been making all my
gazillions using this technique.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;How to Include an Image&lt;/h2&gt;
&lt;p&gt;Fed Chair Powell is working hard:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Picture showing Powell" src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/images/group-Fintech-Disruption_Powell.jpeg"&gt;&lt;/p&gt;</content><category term="Reflective Report"></category><category term="Group Fintech Disruption"></category></entry></feed>