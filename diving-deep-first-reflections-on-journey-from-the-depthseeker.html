<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="FINA4350 Students 2025" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content="Group Depthseeker, Reflective Report, " />

<meta property="og:title" content="Diving Deep - First Reflections on Journey from the DepthSeeker "/>
<meta property="og:url" content="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/diving-deep-first-reflections-on-journey-from-the-depthseeker.html" />
<meta property="og:description" content="First Reflection - Depthseeker Welcome to the DepthSeeker blog, where we document our journey exploring the correlation between social media discussion patterns and cryptocurrency price dynamics. Team Introduction Our interdisciplinary team brings diverse perspectives to this project: Anson (Data Science) - Leading our data cleaning and preprocessing, wherein he could utilize his …" />
<meta property="og:site_name" content="FINA4350 Student Blog 2025" />
<meta property="og:article:author" content="FINA4350 Students 2025" />
<meta property="og:article:published_time" content="2025-03-24T23:30:00+08:00" />
<meta name="twitter:title" content="Diving Deep - First Reflections on Journey from the DepthSeeker ">
<meta name="twitter:description" content="First Reflection - Depthseeker Welcome to the DepthSeeker blog, where we document our journey exploring the correlation between social media discussion patterns and cryptocurrency price dynamics. Team Introduction Our interdisciplinary team brings diverse perspectives to this project: Anson (Data Science) - Leading our data cleaning and preprocessing, wherein he could utilize his …">

        <title>Diving Deep - First Reflections on Journey from the DepthSeeker  · FINA4350 Student Blog 2025
</title>
        <link href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="FINA4350 Student Blog 2025 - Full Atom Feed" />



    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/"><span class=site-name>FINA4350 Student Blog 2025</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://buehlmaier.github.io/FINA4350-student-blog-2025-01
                                    >Home</a>
                                </li>
                                <li ><a href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/categories.html">Categories</a></li>
                                <li ><a href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/tags.html">Tags</a></li>
                                <li ><a href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/archives.html">Archives</a></li>
                                <li><form class="navbar-search" action="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li>
                            </ul>
                        </div>
                    </div>
                </div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/diving-deep-first-reflections-on-journey-from-the-depthseeker.html">
                Diving Deep - First Reflections on Journey from the DepthSeeker
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <h1><em>First Reflection  - Depthseeker</em></h1>
<p>Welcome to the DepthSeeker blog, where we document our journey exploring the correlation between social media discussion patterns and cryptocurrency price dynamics.</p>
<h1><em>Team Introduction</em></h1>
<p>Our interdisciplinary team brings diverse perspectives to this project: </p>
<ul>
<li>Anson (Data Science) - Leading our data cleaning and preprocessing, wherein he could utilize his ability to deal with structured data</li>
<li>Barbie (FinTech) - Helping with the preprocessing of data and taking responsibility for report writing based on her experience in fintech</li>
<li>Woody (Computer Science) - Heading our web scraping activities and handling data visualization, leveraging his programming expertise</li>
<li>Apollo (Economics &amp; Finance) - Providing financial analysis expertise and contributing to sentiment analysis, drawing on his economic knowledge</li>
</ul>
<p>This blend of backgrounds allows us to approach our research from different angles, bringing technical know-how with financial insight. For this particular blog entry, Woody shared his experience with the technical side of data collection, while Barbie helped shape these experiences into a cohesive narrative.</p>
<h1><em>Our Project Motivation: Beyond Trading Strategies</em></h1>
<p>While our first presentation highlighted several practical market applications for our research, including sentiment-driven trading strategies and risk management tools, our ambitions extend far beyond these applications. The group name, "DepthSeeker," captures our true purpose: to dive into the noisy surface of crypto markets and uncover the hidden patterns connecting social sentiment and price movements.</p>
<p>As students with diverse academic backgrounds, we are confronted with the challenge of trying to glean valuable signals from the noisy and sometimes conflicting crypto community. We are thrilled at the prospect of working at the intersection of computer science, behavioral finance, and financial mathematics and discovering areas outside of our respective fields of expertise. In addition to acquiring technical NLP and data analysis abilities, we hope to make academic contributions to cryptocurrency market dynamics research. This project is our learning laboratory where theoretical knowledge meets real-world data problems.</p>
<h1><em>The Unexpected Complexity of Web Scraping</em></h1>
<p>When we first outlined our project timeline, Woody volunteered to carry out the data collection part, which was scraping Reddit and Discord for crypto discussions. What initially seemed easy in theory quickly became a labyrinth of technical challenges.</p>
<h1><em>Discord: A Walled Garden</em></h1>
<p>To perform discord scraping, we need to first find 3 elements</p>
<ul>
<li><em>USER_TOKEN</em>: [could be found in developer tools]</li>
<li><em>SERVER_ID</em>: [after opening the developer mode in discord, right click the server icon]</li>
<li><em>CHANNEL_ID</em>: [after opening the developer mode in discord, right click the channel icon]</li>
</ul>
<p>These elements allow us to scrape a specific channel within a specific server, targeting messages before a specified time.</p>
<div class="highlight"><pre><span></span><code><span class="k">async</span> <span class="k">def</span> <span class="nf">scrape_history</span><span class="p">(</span><span class="n">channel</span><span class="p">,</span> <span class="n">before_time</span><span class="p">,</span> <span class="n">max_batches</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
<span class="w">   </span><span class="sd">&quot;&quot;&quot;Scrape historical messages in batches until done or limit reached.&quot;&quot;&quot;</span>
   <span class="n">total_messages</span> <span class="o">=</span> <span class="mi">0</span>
   <span class="n">batch_count</span> <span class="o">=</span> <span class="mi">0</span>

   <span class="k">while</span> <span class="n">batch_count</span> <span class="o">&lt;</span> <span class="n">max_batches</span><span class="p">:</span>  <span class="c1"># Limit batches to avoid over-scraping</span>
       <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch </span><span class="si">{</span><span class="n">batch_count</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: Scraping before </span><span class="si">{</span><span class="n">before_time</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
       <span class="n">messages_scraped</span> <span class="o">=</span> <span class="mi">0</span>
       <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;crypto_discord_raw.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
           <span class="k">async</span> <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">channel</span><span class="o">.</span><span class="n">history</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">before</span><span class="o">=</span><span class="n">before_time</span><span class="p">):</span>
               <span class="n">messages_scraped</span> <span class="o">+=</span> <span class="mi">1</span>
               <span class="n">total_messages</span> <span class="o">+=</span> <span class="mi">1</span>
               <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">message</span><span class="o">.</span><span class="n">created_at</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">message</span><span class="o">.</span><span class="n">author</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
               <span class="n">f</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
               <span class="k">if</span> <span class="n">messages_scraped</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                   <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch </span><span class="si">{</span><span class="n">batch_count</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: Scraped </span><span class="si">{</span><span class="n">messages_scraped</span><span class="si">}</span><span class="s2"> messages (Total: </span><span class="si">{</span><span class="n">total_messages</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
               <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
           <span class="c1"># Update before_time to the oldest message in this batch</span>
           <span class="k">if</span> <span class="n">messages_scraped</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
               <span class="n">before_time</span> <span class="o">=</span> <span class="n">message</span><span class="o">.</span><span class="n">created_at</span>
           <span class="k">else</span><span class="p">:</span>
               <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;No more messages to scrape.&quot;</span><span class="p">)</span>
               <span class="k">break</span>
       <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Batch </span><span class="si">{</span><span class="n">batch_count</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> complete. Scraped </span><span class="si">{</span><span class="n">messages_scraped</span><span class="si">}</span><span class="s2"> messages.&quot;</span><span class="p">)</span>
       <span class="n">batch_count</span> <span class="o">+=</span> <span class="mi">1</span>
       <span class="k">if</span> <span class="n">messages_scraped</span> <span class="o">&lt;</span> <span class="mi">1000</span><span class="p">:</span>  <span class="c1"># Less than limit means we hit the start</span>
           <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reached the beginning of the channel history.&quot;</span><span class="p">)</span>
           <span class="k">break</span>
       <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>  <span class="c1"># Pause between batches to avoid rate limits</span>

   <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Scraping complete. Total messages scraped: </span><span class="si">{</span><span class="n">total_messages</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>

<p>After the initial setup, I quickly encountered an issue: the current version of the discord.py API is not user-friendly for scraping with a user account. Using a bot for scraping is generally preferred, but since we want to use a user account for simplicity, the latest version poses challenges. To address this, we can use version 1.7.3 of discord.py, which offers easier manipulation for Discord scraping with a user account.</p>
<p>To install this specific version, run the following command in your terminal:</p>
<div class="highlight"><pre><span></span><code><span class="n">pip</span> <span class="n">install</span> <span class="n">discord</span><span class="o">.</span><span class="n">py</span><span class="o">==</span><span class="mf">1.7.3</span>
</code></pre></div>

<p>Woody also discovered a practical limitation with Discord scraping: each request retrieves only about 1,000 messages. This makes selecting a high-quality channel in a reputable server crucial for finding valuable cryptocurrency conversations. Many Discord servers are filled with casual chatter and off-topic noise, diluting crypto-related content. We are still on the hunt for more focused channels that host substantive crypto discussions.</p>
<h1><em>Reddit: API Challenge</em></h1>
<p>To perform Reddit scraping, we first need to create an app within Reddit. This process allows us to obtain the client_id and client_secret, which are essential for initiating the scraping.
Once these credentials are secured, we can begin scraping data, such as the latest posts and comments, using the following approach in our code:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Fetch the next batch of posts</span>
       <span class="n">submissions</span> <span class="o">=</span> <span class="n">subreddit</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;before&#39;</span><span class="p">:</span> <span class="n">last_id</span><span class="p">}</span> <span class="k">if</span> <span class="n">last_id</span> <span class="k">else</span> <span class="p">{})</span>
       <span class="k">for</span> <span class="n">submission</span> <span class="ow">in</span> <span class="n">submissions</span><span class="p">:</span>
           <span class="n">check_counter</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Increment check counter for every post</span>
           <span class="n">current_time</span> <span class="o">=</span> <span class="n">submission</span><span class="o">.</span><span class="n">created_utc</span>

           <span class="c1"># Print every 100th post being checked</span>
           <span class="k">if</span> <span class="n">check_counter</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
               <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Checking post </span><span class="si">{</span><span class="n">submission</span><span class="o">.</span><span class="n">id</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">datetime</span><span class="o">.</span><span class="n">fromtimestamp</span><span class="p">(</span><span class="n">current_time</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

           <span class="k">if</span> <span class="n">current_time</span> <span class="o">&gt;</span> <span class="n">end_date</span><span class="p">:</span>  <span class="c1"># Skip posts after Mar 24, 2025</span>
               <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  -&gt; Skipping (after end_date)&quot;</span><span class="p">)</span>
               <span class="k">continue</span>
           <span class="k">if</span> <span class="n">current_time</span> <span class="o">&lt;</span> <span class="n">start_date</span><span class="p">:</span>  <span class="c1"># Stop if before Mar 14, 2025</span>
               <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  -&gt; Stopping (before start_date)&quot;</span><span class="p">)</span>
               <span class="k">break</span>

           <span class="n">post_count</span> <span class="o">+=</span> <span class="mi">1</span>
           <span class="n">total_posts</span> <span class="o">+=</span> <span class="mi">1</span>
           <span class="n">process_counter</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Increment process counter for every processed post</span>
           <span class="n">batch_posts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">submission</span><span class="p">)</span>

           <span class="c1"># Print every 100th processed post</span>
           <span class="k">if</span> <span class="n">process_counter</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
               <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Processing post #</span><span class="si">{</span><span class="n">total_posts</span><span class="si">}</span><span class="s2">: &#39;</span><span class="si">{</span><span class="n">submission</span><span class="o">.</span><span class="n">title</span><span class="si">}</span><span class="s2">&#39; (ID: </span><span class="si">{</span><span class="n">submission</span><span class="o">.</span><span class="n">id</span><span class="si">}</span><span class="s2">, Time: </span><span class="si">{</span><span class="n">datetime</span><span class="o">.</span><span class="n">fromtimestamp</span><span class="p">(</span><span class="n">current_time</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

           <span class="c1"># Store post data</span>
           <span class="n">posts_data</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
               <span class="s1">&#39;post_id&#39;</span><span class="p">:</span> <span class="n">submission</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
               <span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="n">submission</span><span class="o">.</span><span class="n">title</span><span class="p">,</span>
               <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">submission</span><span class="o">.</span><span class="n">selftext</span><span class="p">,</span>
               <span class="s1">&#39;created_utc&#39;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">fromtimestamp</span><span class="p">(</span><span class="n">current_time</span><span class="p">),</span>
               <span class="s1">&#39;score&#39;</span><span class="p">:</span> <span class="n">submission</span><span class="o">.</span><span class="n">score</span><span class="p">,</span>
               <span class="s1">&#39;num_comments&#39;</span><span class="p">:</span> <span class="n">submission</span><span class="o">.</span><span class="n">num_comments</span><span class="p">,</span>
               <span class="s1">&#39;url&#39;</span><span class="p">:</span> <span class="n">submission</span><span class="o">.</span><span class="n">url</span>
           <span class="p">})</span>

           <span class="c1"># Fetch all comments</span>
           <span class="k">try</span><span class="p">:</span>
               <span class="n">submission</span><span class="o">.</span><span class="n">comments</span><span class="o">.</span><span class="n">replace_more</span><span class="p">(</span><span class="n">limit</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
               <span class="n">comment_count</span> <span class="o">=</span> <span class="mi">0</span>
               <span class="k">for</span> <span class="n">comment</span> <span class="ow">in</span> <span class="n">submission</span><span class="o">.</span><span class="n">comments</span><span class="o">.</span><span class="n">list</span><span class="p">():</span>
                   <span class="n">comment_count</span> <span class="o">+=</span> <span class="mi">1</span>
                   <span class="n">comments_data</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                       <span class="s1">&#39;post_id&#39;</span><span class="p">:</span> <span class="n">submission</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
                       <span class="s1">&#39;comment_id&#39;</span><span class="p">:</span> <span class="n">comment</span><span class="o">.</span><span class="n">id</span><span class="p">,</span>
                       <span class="s1">&#39;body&#39;</span><span class="p">:</span> <span class="n">comment</span><span class="o">.</span><span class="n">body</span><span class="p">,</span>
                       <span class="s1">&#39;created_utc&#39;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">fromtimestamp</span><span class="p">(</span><span class="n">comment</span><span class="o">.</span><span class="n">created_utc</span><span class="p">),</span>
                       <span class="s1">&#39;score&#39;</span><span class="p">:</span> <span class="n">comment</span><span class="o">.</span><span class="n">score</span><span class="p">,</span>
                       <span class="s1">&#39;parent_id&#39;</span><span class="p">:</span> <span class="n">comment</span><span class="o">.</span><span class="n">parent_id</span>
                   <span class="p">})</span>
               <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  -&gt; Fetched </span><span class="si">{</span><span class="n">comment_count</span><span class="si">}</span><span class="s2"> comments for post </span><span class="si">{</span><span class="n">submission</span><span class="o">.</span><span class="n">id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
           <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
               <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  -&gt; Error fetching comments for post </span><span class="si">{</span><span class="n">submission</span><span class="o">.</span><span class="n">id</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

           <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Respect rate limits</span>
</code></pre></div>

<p>Our Reddit data acquisition method hit an unexpected roadblock when Woody uncovered major API changes introduced by Reddit in 2023. Specifically, PRAW (Python Reddit API Wrapper) no longer supports retrieving posts between two specific dates—a feature removed starting with version 6.0.0. Consequently, our current scraping approach is restricted to fetching only the latest posts, as the official API’s free tier no longer supports time-based search queries.
To address this limitation, we’re taking the following steps:</p>
<ul>
<li>We are actively exploring alternative methods to access historical Reddit data from specific time periods, which would enhance our historical analysis capabilities.</li>
<li>In parallel, we are building our own cryptocurrency price dataset, ensuring the dates align with our text data for more cohesive analysis.</li>
</ul>
<h1><em>Cryptocurrency Price Data</em></h1>
<p>After reviewing datasets available on Kaggle and Hugging Face, Woody determined that most were too outdated for our needs. To overcome this, we turned to direct API calls to Coinbase, which provide historical data for any cryptocurrency we’re interested in, across any timeframe. This approach also opens the door to incorporating real-time pricing into future project implementations.</p>
<p>For example, we can use the following API call to retrieve spot prices with a historical date parameter:
response = requests.get(f'https://api.coinbase.com/v2/prices/{coin_pair}/spot?date={date}')</p>
<p>Note that only the spot price endpoint supports the date parameter for historical price requests.</p>
<p>To illustrate, if we want to fetch BTC/USD pricing data from one year ago, we can structure the request like this:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>


<span class="c1"># Define the start and end dates for the previous year</span>
<span class="n">end_date</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">year</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">month</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">day</span><span class="o">=</span><span class="mi">31</span><span class="p">)</span>
<span class="n">start_date</span> <span class="o">=</span> <span class="n">end_date</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">year</span><span class="o">=</span><span class="n">end_date</span><span class="o">.</span><span class="n">year</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>


<span class="c1"># Define the list of dates to fetch</span>
<span class="n">date_list</span> <span class="o">=</span> <span class="p">[(</span><span class="n">start_date</span> <span class="o">+</span> <span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">end_date</span> <span class="o">-</span> <span class="n">start_date</span><span class="p">)</span><span class="o">.</span><span class="n">days</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
<span class="n">coin_pair</span> <span class="o">=</span> <span class="s1">&#39;BTC-USD&#39;</span>
<span class="c1"># Prepare CSV file</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;btc_historical_data.csv&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">newline</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
   <span class="n">writer</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">writer</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
   <span class="c1"># Write header</span>
   <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">([</span><span class="s2">&quot;Date&quot;</span><span class="p">,</span> <span class="s2">&quot;Amount&quot;</span><span class="p">,</span> <span class="s2">&quot;Base&quot;</span><span class="p">,</span> <span class="s2">&quot;Currency&quot;</span><span class="p">])</span>

   <span class="k">for</span> <span class="n">date</span> <span class="ow">in</span> <span class="n">date_list</span><span class="p">:</span>
       <span class="c1"># Make API request for each date</span>
       <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;https://api.coinbase.com/v2/prices/</span><span class="si">{</span><span class="n">coin_pair</span><span class="si">}</span><span class="s1">/spot?date=</span><span class="si">{</span><span class="n">date</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
       <span class="n">data</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>


       <span class="c1"># Extract necessary information</span>
       <span class="n">amount</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">][</span><span class="s1">&#39;amount&#39;</span><span class="p">]</span>
       <span class="n">writer</span><span class="o">.</span><span class="n">writerow</span><span class="p">([</span><span class="n">date</span><span class="p">,</span> <span class="n">amount</span><span class="p">,</span> <span class="s1">&#39;BTC&#39;</span><span class="p">,</span> <span class="s1">&#39;USD&#39;</span><span class="p">])</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CSV file has been created successfully.&quot;</span><span class="p">)</span>
</code></pre></div>

<h1><em>Lesson Learnt</em></h1>
<p>We have already learned several valuable lessons during this initial stage of the project. What began as a straightforward data collection task evolved into a difficult engineering challenge that considerably pushed our technical abilities forward. We discovered that API documentation literacy is essential, as a thorough reading of platform documentation before implementation would have spared countless hours of debugging and redevelopment. Our Discord experience taught us that specific library versions can distinguish between success and failure, highlighting the importance of dependency management.</p>
<p>We also learned to maximize value from limited computational and financial resources. Strategic data sampling from authoritative sources yielded better results than processing large volumes of low-quality content. These resource optimization techniques proved essential, given our academic project constraints.</p>
<p>These challenges ultimately strengthened our research methodology. Small-scale pilot testing revealed limitations in our approach before we committed further resources to potentially flawed strategies, allowing us to course-correct early and build a more robust analytical framework.</p>
<h1><em>Looking Forward</em></h1>
<p>As we move into the next phase of our project, we will focus on identifying and integrating higher-quality data sources while implementing rigorous data cleansing processes. Our preprocessing pipeline will standardize the dataset by removing irrelevant characters, punctuation, and stop words. We foresee it as particularly challenging owing to the high noise inherent in social media text data. This racket, made up of emojis, platform-specific slang, and intentional misspellings, presents obstacles to correct sentiment analysis. Nevertheless, we recognize that comprehensive cleaning and preprocessing are not merely technical necessities but fundamental prerequisites for coherent signal extraction from the untamed landscape of cryptocurrency discussion online.</p>
<p>Stay tuned for our next blog post, where we will share our experience working on the project!</p>
<p><em>(Word Count: 1140)</em></p>


             
 
            
            
            







            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2025-03-24T23:30:00+08:00">Mon 24 March 2025</time>
            <h4>Category</h4>
            <a class="category-link" href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/categories.html#reflective-report-ref">Reflective Report</a>
            <h4>Tags</h4>
            <ul class="list-of-tags tags-in-article">
                <li><a href="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/tags.html#group-depthseeker-ref">Group Depthseeker
                    <span class="superscript">1</span>
</a></li>
            </ul>
<h4>Contact</h4>
<div id="sidebar-social-link">
    <a href="https://github.com/buehlmaier/FINA4350-student-blog-2025-01" title="" target="_blank" rel="nofollow noopener noreferrer">
        <svg xmlns="http://www.w3.org/2000/svg" aria-label="GitHub" role="img" viewBox="0 0 512 512"><rect width="512" height="512" rx="15%" fill="#1B1817"/><path fill="#fff" d="M335 499c14 0 12 17 12 17H165s-2-17 12-17c13 0 16-6 16-12l-1-50c-71 16-86-28-86-28-12-30-28-37-28-37-24-16 1-16 1-16 26 2 40 26 40 26 22 39 59 28 74 22 2-17 9-28 16-35-57-6-116-28-116-126 0-28 10-51 26-69-3-6-11-32 3-67 0 0 21-7 70 26 42-12 86-12 128 0 49-33 70-26 70-26 14 35 6 61 3 67 16 18 26 41 26 69 0 98-60 120-117 126 10 8 18 24 18 48l-1 70c0 6 3 12 16 12z"/></svg>
    </a>
</div>
            





            





        </section>
</div>
</article>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="https://buehlmaier.github.io/FINA4350-student-blog-2025-01/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>

    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>